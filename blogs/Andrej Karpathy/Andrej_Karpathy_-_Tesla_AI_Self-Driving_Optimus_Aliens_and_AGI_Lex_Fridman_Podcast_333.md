---
layout: default
title: Andrej Karpathy - Tesla AI Self-Driving Optimus Aliens and AGI Lex Fridman Podcast 333
parent: Andrej Karpathy
has_children: false
nav_order: 7
---

# Uncovering the Exploits of Physics and the Power of Neural Networks

I believe it's possible that physics has exploits, and we should be actively trying to find them. Imagine arranging some kind of a crazy quantum mechanical system that somehow gives you buffer overflow, or a rounding error in the floating point. 

Synthetic intelligence is kind of like the next stage of development. I'm not sure where it leads to, but at some point, I suspect the universe is some kind of a puzzle. These synthetic AIs will uncover that puzzle and solve it.

Today, we have a conversation with Andrej Karpathy, previously the director of AI at Tesla. Before that, he was at OpenAI and Stanford. He is one of the greatest scientists, engineers, and educators in the history of artificial intelligence. 

## What is a Neural Network?

So, what is a neural network and why does it seem to do such a surprisingly good job of learning? 

A neural network is a mathematical abstraction of the brain. That's how it was originally developed. At the end of the day, it's a mathematical expression and it's a fairly simple one when you get down to it. It's basically a sequence of matrix multipliers, or dot products mathematically, with some non-linearities thrown in. 

It's a very simple mathematical expression with knobs in it. Many knobs. These knobs are loosely related to the synapses in your brain. They're trainable. They're modifiable. The idea is we need to find the setting of the knobs that makes the neural net do whatever you want it to do, like classify images and so on. 

There's not too much mystery in it. You might think that you don't want to endow it with too much meaning with respect to the brain and how it works. It's really just a complicated mathematical expression with knobs. And those knobs need a proper setting for it to do something desirable.

But poetry is just a collection of letters with spaces, yet it can make us feel a certain way. In that same way, when you get a large number of knobs together, whether it's inside the brain or inside a computer, they seem to surprise us with their power.

I think I'm underselling it by a lot because you definitely do get very surprising emergent behaviors out of these neural nets when they're large enough and trained on complicated enough problems. Like say, for example, the next-word prediction in a massive dataset from the internet. Then these neural nets take on pretty surprising magical properties.
# The Power of Neural Networks: A Conversation

It's interesting how much you can get out of even very simple mathematical formalism. When your brain is talking right now, is it doing next-word prediction or is it doing something more interesting?

Well, it's definitely some kind of a generative model that's GPT-like and prompted by you. So you're giving me a prompt and I'm kind of like responding to it in a generative way. 

And by yourself perhaps a little bit, like are you adding extra prompts from your own memory inside your head or no? Well, it definitely feels like you're referencing some kind of a declarative structure of memory and so on. And then you're putting that together with your prompt and giving away some answers.

How much of what you just said has been said by you before? Nothing, basically right? No, but if you actually look at all the words you've ever said in your life and you do a search, you'll probably have said a lot of the same words in the same order before. 

Yeah, could be. I mean, I'm using phrases that are common, et cetera, but I'm remixing it into a pretty unique sentence at the end of the day. But you're right, definitely, there's like a ton of remixing. 

It's like Magnus Carlson said I'm rated 2,900, whatever, which is pretty decent. I think you're talking very, you're not giving enough credit to neural nets here. What's your best intuition about this emergent behavior?

I mean, it's kind of interesting because I'm simultaneously underselling them, but I also feel like there's an element to which I'm over- it's actually kind of incredible that you can get so much emergent magical behavior out of them despite them being so simple mathematically. 

So I think those are two surprising statements that are juxtaposed together. And I think, basically, what it is, is we are actually fairly good at optimizing these neural nets. And when you give them a hard enough problem, they are forced to learn very interesting solutions in the optimization. And those solutions basically have these emergent properties that are very interesting. 

There's wisdom and knowledge in the knobs. And so this representation that's in the knobs does it make sense to you intuitively, that a large number of knobs can hold a representation that captures some deep wisdom about the data it has looked at. It's a lot of knobs.

It's a lot of knobs. And somehow, so speaking concretely, one of the neural nets that people are very excited about right now are GPTs, which are basically just next-word prediction networks. So you consume a sequence of words from the internet and you try to predict the next word. 

And once you train these on a large enough data set, you can basically prompt these neural nets in arbitrary ways and you can...
# Neural Networks: A Discussion on Problem Solving and Biological Computation

Ask them to solve problems, and they will. You can just tell them, you can make it look like you're trying to solve some kind of a mathematical problem. They will continue what they think is the solution based on what they've seen on the internet. Very often, those solutions look remarkably consistent and potentially even correct.

Do you still think about the brain side of it? As neural nets as an abstraction, a mathematical abstraction of the brain, do you still draw wisdom from the biological neural networks or even the bigger question? You're a big fan of biology and biological computation. What impressive thing is biology doing to you that computers are not yet, that gap?

I would say I'm definitely on, I'm much more hesitant with the analogies to the brain than I think you would see potentially in the field. I feel like certainly, the way neural networks started is everything stemmed from inspiration by the brain. But at the end of the day, the artifacts that you get after training, they are arrived at by a very different optimization process than the optimization process that gave rise to the brain. 

I think of it as a very complicated alien artifact. It's something different. The neural nest that we're training, they are a complicated alien artifact. I do not make analogies to the brain because I think the optimization process that gave rise to it is very different from the brain. So there was no multi-agent, self-play setup and evolution. It was an optimization that is basically what amounts to a compression objective on a mass amount of data.

Artificial neural networks are doing compression and biological neural networks are trying to survive. They're an agent in a multi-agent, self-play system that's been running for a very, very long time. 

That said, evolution has found that it is very useful to predict and have a predictive model in the brain. Our brain utilizes something that looks like that as a part of it, but it has a lot more catches and gizmos and value functions and ancient nuclei that are all trying to make it survive and reproduce and everything else.

And the whole thing through embryogenesis is built from a single-cell. I mean, it's just the code is inside the DNA and it just builds it up like the entire organism with arms and the head and legs. It's definitely crazy. It should not be possible. 

So there's some learning going on. There's some kind of computation going through that building process. I mean, I don't know where, if you were just to look at the entirety of history of life on earth.
# The Most Interesting Invention: A Discussion on the Origin of Life and Intelligence

Where do you think is the most interesting invention? Is it the origin of life itself? Or perhaps the leap to Eukaryotes? Maybe it's the emergence of mammals, or even humans themselves, Homo sapiens? Could it be the origin of intelligence or highly complex intelligence? Or is it all just a continuation of the same kind of process?

Certainly, I would say it's an extremely remarkable story that I'm only briefly learning about recently. The story spans all the way from the formation of Earth and all of its conditions, the entire solar system, and how everything is arranged with Jupiter, the moon, and the habitable zone. Then you have an active Earth that's turning over material, and then you start with abiogenesis. So, it's all a pretty remarkable story.

I'm not sure that I can pick a single unique piece of it that I find most interesting. As an artificial intelligence researcher, it's probably the last piece that intrigues me the most. We have lots of animals that are not building a technological society, but we do. And it seems to have happened very quickly and very recently. Something very interesting happened there that I don't fully understand. I almost understand everything else intuitively, but I don't understand exactly that part and how quick it was.

Both explanations would be interesting. One is that this is just a continuation of the same kind of process. There's nothing special about humans. Deeply understanding that would be very interesting. We think of ourselves as special, but it was already written in the code that you would have greater and greater intelligence emerging.

The other explanation is that something truly special happened, something like a rare event. Whether it's a crazy rare event like a "Space Odyssey", the invention of fire, or as Richard Rankin says, the beta males deciding a clever way to kill the alpha males by collaborating. So just optimizing the collaboration, the multi-agent aspect of the multi-agent, and that really being constrained on resources and trying to survive, the collaboration aspect is what created the complex intelligence. But it seems like it's a natural algorithm to the evolution process.

What could possibly be a magical thing that happened, like a rare thing that would say that humans are actually, human-level intelligence is actually a really rare thing in the universe? I'm hesitant to say that it is rare, but it definitely seems like it's like a punctuated equilibrium where you have lots of exploration and then you have certain leaps, sparse leaps in between.

Of course, like the origin of life would be one, DNA, sex, Eukaryotic life, the endosymbiosis event where the archon ate little bacteria, just the whole thing. And then of course, the emergence of complex intelligence.
# The Fermi Paradox and the Origin of Life

The question of consciousness and the uniqueness of human intelligence has always intrigued me. There have been sparse events in history where a massive amount of progress was made in understanding these concepts. However, it's kind of hard to pick one event that stands out the most.

One might ask, "Do you think humans are unique?" To that, I would counter, "How many intelligent alien civilizations do you think are out there? And is their intelligence different or similar to ours?"

I've been preoccupied with this question quite a bit recently. The Fermi paradox, which questions the apparent contradiction between the lack of evidence for extraterrestrial civilizations and various high estimates for their probability, has been a subject of my contemplation. 

The reason I am very interested in the origin of life is that I am fundamentally trying to understand how common it is for there to be technological societies out there in space. The more I study it, the more I think that there should be quite a few, quite a lot.

One might wonder, "Why haven't we heard from them?" I agree with this sentiment. It feels like what we did here on earth is not so difficult to replicate. 

I used to think the origin of life was a magical rare event. But then I read books like "The Vital Question" and "Life Ascending" by Nick Lane. He really gets in and makes you believe that the origin of life is not that rare. 

You have an active earth, alkaline vents, and lots of alkaline waters mixing where there is a devotion. You have your proton gradients and these little porous pockets of these alkaline vents that concentrate chemistry. As he steps through all of these little pieces, you start to understand that this is not that crazy. You could see this happen on other systems. He really takes you from just a geology to primitive life. He makes it feel like this is actually pretty plausible.

The origin of life was actually fairly fast after the formation of earth, just a few hundred million years or something like that after basically when it was possible, life actually arose. That makes me feel like that is not the constraint, that is not the limiting variable, and that life should actually be fairly common. 

Where the drop-offs are is very interesting to think about. I currently think that there are no major drop-offs basically. And so there should be quite a lot of life. 

Where that brings me to then is the only way to reconcile the fact that we haven't found anyone and so on is that we can't see them, we can't observe them.

Just a quick brief comment, Nick Lane and a lot of biologists I talk to, they really seem to think that the jump from bacteria to more complex organisms is the hardest jump. The transition to Eukaryotic life, basically. I get it. They're much more knowledgeable than me.
# The Intricacies of Biology and the Search for Alien Life

The intricacies of biology can seem overwhelming. It might seem crazy to think about the sheer number of single-cell organisms that exist. Given the amount of time we have, it surely can't be that difficult to understand them. A billion years isn't even that long in the grand scheme of things. Just consider all the bacteria under constrained resources battling it out. I'm sure they can invent more complex organisms. 

It's like trying to move from a "Hello, World!" program to inventing a function or something similar. I just feel like I don't see any reason why, if the origin of life isn't the hardest thing to understand, it can't be everywhere. Maybe we're just too dumb to see it. 

The problem is, we don't have really good mechanisms for detecting this life. I'm not an expert, but from what I understand, our ability to find these intelligences out there and to find earth-like planets is limited. Radio waves, for example, are terrible for this purpose. Their power drops off as basically 1 over R-squared. 

I remember reading that our current radio waves, the ones that we are broadcasting, would not be measurable by our devices today only a short distance away. You really need a targeted transmission of massive power directed somewhere for this to be picked up over long distances. 

I think there's probably other civilizations out there. The big question is, why don't they build one-man probes and why don't they engage in interstellar travel across the entire galaxy? My current answer is, it's probably because interstellar travel is really hard. 

If you want to move at close to the speed of light, you're going to encounter bullets along the way. Even tiny hydrogen atoms and little particles of dust have massive kinetic energy at those speeds. You need some kind of shielding, and you have to deal with all the cosmic radiation. It's just brutal out there. 

My thinking is maybe interstellar travel is just extremely hard. You have to learn to move slowly. It might not be a billion years away from doing that, but it just might be that you have to go very slowly through space. 

So, I'm suspicious of our ability to measure life and I'm suspicious of the ability to just permeate all of space in the galaxy or across galaxies.
# A Conversation on Alien Civilizations and Complex Dynamical Systems

"And that's the only way that I can currently see a way around it," one of us said. The idea is kind of mind-blowing, to think that there are trillions of intelligent alien civilizations out there, slowly traveling through space. 

"Maybe," Andrej replied. 

The thought of these civilizations meeting each other is fascinating. Some of them meet, some of them go to war, some of them collaborate. Or perhaps they're all just independent. They're all just like little pockets. It's hard to know for sure. 

Well, statistically, if there are trillions of them, surely some of them, some of the pockets, are close enough together. Some of them happen to be close. And close enough to see each other. 

Once you see something that is definitely complex life, if we see something, we're probably going to be intensely, aggressively motivated to figure out what the hell that is and try to meet them. But what would be your first instinct? To try to, at a generational level, meet them or defend against them? Or what would be your instinct as a president of the United States and a scientist? 

The question is really hard. For example, we have lots of primitive life forms on earth next to us. We have all kinds of ants and everything else, and we share a space with them. We are hesitant to impact them and we're trying to protect them by default, because they are amazing, interesting dynamical systems that took a long time to evolve. They are interesting and special and I don't know that you want to destroy that by default. 

I like complex dynamical systems that took a lot of time to evolve. I think I'd like to preserve it if I can afford to. And I'd like to think that the same would be true about the galactic resources and that they would think that we're kind of an incredible, interesting story that took time, a few billion years, to unravel and you don't want to just destroy it. 

I could see two aliens talking about earth right now and saying, "I'm a big fan of complex dynamical systems. So I think it's of value to preserve these," and who basically are watching us like a video game or a TV show. 

I think you would need a very good reason to destroy it. Why don't we destroy these ant farms and so on? It's because we're not actually in direct competition with them right now. We do it accidentally and so on, but there's plenty of resources. And so why would you destroy something that is so interesting and precious? 

From a scientific perspective, you might probe it. You might interact with it lightly. You might want to learn something from it. So I wonder, there could be...
# A Conversation on Life, Computation, and Alien Scientists

There are certain physical phenomena that we perceive as just that, physical phenomena. However, these phenomena could actually be interacting with us in ways we don't fully understand yet. It's like poking a finger into the unknown and observing what happens.

This concept should be very interesting to scientists, including hypothetical alien scientists. What we're observing today is essentially a snapshot, a result of a massive amount of computation that has been ongoing for billions of years.

One could even speculate that this could have been initiated by aliens. Perhaps what we're experiencing is a computer running a complex program. If we had the power to do this, wouldn't we? I know I would. I would select an earth-like planet with the right conditions and base my understanding on the chemistry prerequisites for life. Then, I would seed it with life and let it run its course.

Wouldn't you want to observe and protect such an experiment? It's not just an incredibly engaging TV show, it's a valuable scientific experiment and a physical simulation. Perhaps running evolution is the most efficient way to understand computation or to comprehend life and the various paths it can take.

This idea does make me feel a bit strange, as if we're part of a science experiment. But then again, isn't everything a science experiment in some way? Does it change anything for us if we're part of a science experiment? Two descendants of apes discussing the possibility of being inside a science experiment is quite a thought.

However, I'm skeptical of this idea of deliberate panspermia. I don't see any divine intervention in the historical record right now. The story in books like Nick Lane's makes sense to me, and it seems plausible how life arose on earth uniquely. I don't feel the need to reach for more exotic explanations at this point.

But consider this: Non-Player Characters (NPCs) inside a video game don't observe any divine intervention either. We might just be NPCs running a code. Maybe once they're running advanced programs like GPTs, they'll start questioning their existence.

I once tweeted, "It looks like if you bombard earth with photons for a while, you can emit a Roadster." If we were to summarize the story of earth, like in "Hitchhiker's Guide to the Galaxy" where earth is described as 'mostly harmless', what would it be? What are all the possible stories that could summarize earth once it's done its computation?

There's going to be an end to earth, and it could end in all kinds of ways. It could even end soon. But until then, we continue to observe, learn, and question.
# A Conversation on the Evolution of Intelligence

**Andrej**: It can end later. What do you think are the possible stories?

Well, there definitely seems to be a sort of incredible phenomenon where these self-replicating systems arise from the dynamics and then they perpetuate themselves, becoming more complex. Eventually, they become conscious and build a society. I feel like, in some sense, it's a deterministic wave that just happens on any sufficiently well-arranged system like Earth. So, I feel like there's a certain sense of inevitability in it and it's really beautiful.

**Lex**: And it ends somehow, right? So it's a chemically diverse environment where complex dynamical systems can evolve and become more complex. But then there's a certain, what is it? There are certain terminating conditions.

**Andrej**: Yeah, I don't know what the terminating conditions are, but definitely, there's a trend line of something and we're part of that story. Where does that go? We're famously described often as a biological boot loader for AIs. That's because humans, I mean, we're an incredible biological system and we're capable of computation and love and so on, but we're extremely inefficient as well. We're talking to each other through audio. It's just embarrassing honestly that we're manipulating like seven symbols, serially, we're using vocal chords. It's all happening over multiple seconds.

**Lex**: Yeah.

**Andrej**: It's just embarrassing when you step down to the frequencies at which computers operate or are able to cooperate on. So basically, it does seem like synthetic intelligences are the next stage of development. I don't know where it leads to, at some point I suspect the universe is some kind of a puzzle and these synthetic AIs will uncover that puzzle and solve it.

**Lex**: And then what happens after, right? 'Cause if you just fast forward Earth, many billions of years, it's quiet and then it's turmoil, you see city lights and stuff like that. And then what happens at the end? Is it a calming, is it an explosion? Is it like Earth opening like a giant? 'Cause you said emit Roadsters. Will it start emitting a giant number of satellites?

**Andrej**: Yeah. Some kind of a crazy explosion. And we're living, we're stepping through an explosion and we're living day-to-day and it doesn't look like it. But it's actually, I saw a very cool animation of Earth and life on Earth and, basically, nothing happens for a long time. And then the last like two seconds, basically cities and everything and just, and the lower orbit just gets cluttered and the whole thing happens in the last two seconds and you're like, "This is exploding. This is a state of explosion."

**Lex**: Yeah, yeah. If you play it at normal speed.

**Andrej**: Yeah. It'll just look like an explosion. It's a firecracker. We're living in a firecracker.
# A Discussion on the Universe: A Constructive Firecracker

"Where it's going to start emitting all kinds of interesting things," one of us begins. 

"Yeah," Andrej agrees. "And then, so an explosion doesn't necessarily mean destruction. It might actually look like a little explosion with lights, fire, and energy emitted, all that kind of stuff. But when you look inside the details of the explosion, there's actual complexity happening where there's like, yeah, human life or some kind of life. We hope it's not a destructive firecracker. It's kind of like a constructive firecracker."

Lex chimes in, "All right. So given that, I think it is really interesting to think about what the puzzle of the universe is. Did the creator of the universe give us a message? For example, in the book 'Contact' by Carl Sagan, there's a message for any civilization in the digits in the expansion of Pi and base 11, eventually, which is kind of an interesting thought. Maybe we're supposed to be giving a message to our creator, maybe we're supposed to somehow create some kind of a quantum mechanical system that alerts them to our intelligent presence here. 'Cause if you think about it from their perspective, it's just say like quantum field theory, a massive cellular automaton-like thing. And how do you even notice that we exist? You might not even be able to pick us up in that simulation. And so how do you prove that you exist, that you're intelligent, and that you're part of the universe?"

"So this is like a Turing test for intelligence from Earth," Andrej suggests. 

"Yeah," Lex agrees. "I mean maybe this is like trying to complete the next word in a sentence. This is a complicated way of that. Earth is basically sending a message back. The puzzle is basically alerting the creator that we exist. Or maybe the puzzle is just to just break out of the system and just stick it to the creator in some way. Basically, like if you're playing a video game, you can somehow find an exploit and find a way to execute on the host machine in arbitrary code. For example, I believe someone got a game of Mario to play pong just by exploiting it and then basically writing code and being able to execute arbitrary code in the game. And so maybe we should be, maybe that's the puzzle is that we should find a way to exploit it. So, I think like some of these synthetic AIs will eventually find the universe to be some kind of a puzzle and then solve it in some way. And that's kind of like the end game somehow."

"Do you often think about it as a simulation?" Andrej asks. "So as are the universe being a kind of computation that might have bugs and exploits?"

"Yes," Lex responds. "Yeah, I think so. Is that what physics is, essentially? I think it's possible that physics has exploits and we should be trying to find them. Arranging some kind of a crazy quantum mechanical system."
# AI and the Universe: A Discussion

"That somehow gives you a buffer overflow, somehow gives you a rounding error and the floating point," one of the speakers said. 

"Yeah, that's right," the other agreed. "And more and more sophisticated exploits, those are jokes, but that could be actually very close to reality."

"We'll find some way to extract infinite energy," the first speaker continued. "For example, when you train reinforcement learning agents in physical simulations and you ask them to say, run quickly on the flat ground, they'll end up doing all kinds of weird things in part of that optimization, right? They'll get on their back leg and they'll slide across the floor. And it's because the optimization, the enforcement learning optimization on that agent has figured out a way to extract infinite energy from the friction forces and, basically, their poor implementation. And they found a way to generate infinite energy and just slide across the surface. And it's not what you expected, it's sort of like a perverse solution."

"And so maybe we can find something like that," the second speaker suggested. "Maybe we can be that little dog in this physical simulation."

"That cracks or escapes the intended consequences of the physics that the universe came up with," the first speaker added.

"We'll figure out some kind of shortcut to some weirdness," the second speaker proposed.

"And then, oh man, but see the problem with that weirdness is the first person to discover the weirdness, like sliding on the back legs, that's all we're gonna do. It's very quickly become everybody does that thing. So the paperclip maximizer is a ridiculous idea, but that very well could be what then we'll just all switch to that 'cause it's so fun."

"Well, no person will discover it, I think by the way," the first speaker interjected. "I think it's going to have to be some kind of a super-intelligent AGI of a third generation. We're building the first-generation AGI, maybe."

"Third generation. Yeah. So the boot loader for an AI, that AI will be a boot loader for another AI. And then there's no way for us to introspect what that might even be."

"I think it's very likely that these things, for example, say you have these AGIs, it's very likely for example, they will be completely inert. I like these kinds of sci-fi books sometimes where these things are just completely inert, they don't interact with anything. And I find that kind of beautiful because they've probably figured out the meta-game of the universe in some way, potentially. They're doing something completely beyond our imagination and they don't interact with simple chemical life forms. Why would you do that? So I find those kinds of ideas compelling."

"What's their source of fun? What are they doing? What's the source of pleasure?" the second speaker asked.

"Well, probably puzzle-solving in the universe?" the first speaker suggested.
# The Inertness of Quantum Mechanics and the Deterministic Universe

"But what does it mean to be inert?" you might ask. Inertness, in this context, refers to the ability to escape interaction with physical reality. To us, these entities will appear inert, behaving in ways that may seem strange because they are beyond our comprehension. They are playing the meta-game.

The meta-game could involve arranging quantum mechanical systems in peculiar ways to extract infinite energy or solve the digital expansion of Pi to any given amount. They might even build their own miniature fusion reactors. Whatever they are doing, it is beyond our comprehension, yet brilliant in its own right.

What if quantum mechanics itself is the system? We might think it's physics, but what if we are merely living on this organism, trying to understand it? Perhaps physics itself is the organism with a deep, profound intelligence. It could be doing something incredibly interesting, and we are just a small part of it.

We are like ants sitting on top of it, trying to extract energy. We are particles in a wave that is mostly deterministic, taking the universe from some kind of a big bang to a super-intelligent replicator, a stable point in the universe given these laws of physics.

Einstein once said, "God doesn't play dice." So, is the universe mostly deterministic? Is there no randomness in it? I believe it is deterministic. There is a lot of what appears to be randomness, but I am cautious with that term. I don't like the idea of randomness. I think the laws of physics are deterministic.

The question of whether the universe is random or not can be unsettling. I believe it's a deterministic system. Things that appear random, like the collapse of the wave function, are actually deterministic. This could be due to entanglement and some kind of multiverse theory.

So why does it feel like we have free will? If I raise my hand, it doesn't feel like a deterministic action. It feels like I'm making a choice. But it's just that - a feeling. When our RL agent makes a choice, it's not really making a choice. The choice was already there.
# The Most Beautiful Idea in AI: The Transformer Architecture

"Yes, you're interpreting the choice and you're creating a narrative for having made it. And now we're talking about the narrative, it's very meta. Looking back, what is the most beautiful or surprising idea in deep learning or AI in general that you've come across? You've seen this field explode and grow in interesting ways. Just what cool ideas, like what made you sit back and go hmm, big or small?"

"Well, the one that I've been thinking about recently the most probably is the transformer architecture. So basically, neural networks have a lot of architectures that were trendy have come and gone for different sensory modalities. Like for vision, audio, text, you would process them with different-looking neural nets. And recently, we've seen this convergence towards one architecture, the transformer, and you can feed it video, or you can feed it images, or speech or text and it just gobbles it up. And it's a bit of a general-purpose computer that is also trainable and very efficient to run in our hardware. And so this paper came out in 2016, I want to say."

"[Lex] "Attention is all you need".

"You criticized the paper title in retrospect that it wasn't, it didn't foresee the bigness of the impact that it was going to have."

"Yeah. I'm not sure if the authors were aware of the impact that that paper would go on to have. Probably they weren't but I think they were aware of some of the motivations and design decisions behind the transformer and they chose not to, I think, expand on it in that way in the paper. And so I think they had an idea that there was more than just the surface of just like, oh, we're just doing translation and here's a better architecture. You're not just doing translation. This is like a really cool differentiable, optimizable, efficient computer that you've proposed. And maybe they didn't have all of that foresight but I think it's really interesting."

"Isn't it funny, sorry to interrupt, that that title is memeable, that they went for such a profound idea. I don't think anyone used that kind of title before, right?"

"Attention is all you need? Yeah. It's like a meme or something, basically."

"Yeah. Isn't it funny that when, maybe if it was a more serious title it wouldn't have the impact."

"Honestly, yeah, there is an element of me that honestly agrees with you and prefers it this way."

"[Lex] Yes.

"If it was too grand it would over-promise and then under-deliver potentially. So you want to just meme your way to greatness?"

"That should be a T-shirt. So you Tweeted, "The Transformer is a magnificent neural network architecture because it is a general-purpose differentiable computer. It is simultaneously..."
# Discussion on Expressive, Optimizable, and Efficient Design Criteria in Transformers

In this discussion, we delve into the details of the design criteria that make transformers successful. These criteria include being expressive in the forward pass, optimizable via back-propagation, and efficient in high parallelism compute graph.

A general-purpose computer that can be trained on arbitrary problems, such as next-word prediction or detecting if there's a cat in an image, is the ideal goal. To achieve this, we need to set the computer's weights. There are several design criteria that overlap in the transformer simultaneously, making it very successful. The authors of the transformer were deliberately trying to create this powerful architecture.

The transformer is very powerful in the forward pass because it's able to express very general computation as something that looks like message passing. You have nodes that store vectors, and these nodes get to look at each other's vectors and communicate. Nodes get to broadcast, "Hey, I'm looking for certain things." Other nodes get to broadcast, "Hey, these are the things I have." These are the keys and the values.

However, the transformer is much more than just the attention component. It has many architectural pieces that went into it, such as the residual connection, the way it's arranged, the multilayer perceptron, and the way it's stacked. Essentially, there's a message-passing scheme where nodes get to look at each other, decide what's interesting, and then update each other.

When you get to the details of it, the transformer is a very expressive function. It can express lots of different types of algorithms in a forward pass. Not only that, but the way it's designed with the residual connections, layer normalizations, the Softmax, attention, and everything, it's also optimizable. This is a really big deal because there are lots of computers that are powerful but not easy to optimize using the techniques that we have, which are back-propagation and gradient descent. These are first-order methods, very simple optimizers.

Lastly, you want it to run efficiently on our hardware. Our hardware, like GPUs, is a massive throughput machine that prefers lots of parallelism. So, you don't want to do lots of sequential operations; you want to do a lot of operations serially. The transformer is designed with that in mind as well. It's designed for our hardware and is designed to both be very expressive in a forward pass and very optimizable in the backward pass.

The residual connection support in the transformer provides an ability to learn short algorithms fast and first.
# Understanding the Resilience of Transformer Architecture

In the world of machine learning, the transformer architecture is a series of blocks. These blocks consist of attention and a small multilayer perceptron. The structure is such that you go off into a block and then come back to this residual pathway. This process is repeated, with a number of layers arranged sequentially.

The residual pathway plays a crucial role during the backward pass. Due to the residual pathway, the gradients flow along it uninterrupted because addition distributes the gradient equally to all of its branches. The gradient from the supervision at the top just floats directly to the first layer. All these residual connections are arranged so that in the beginning, during initialization, they contribute nothing to the residual pathway.

To better understand this, imagine the transformer as a Python function. If you have a transformer that is a hundred layers deep, although typically they would be much shorter, say 20, you have 20 lines of code where you can do something. During the optimization, you first optimize the first line of code, then the second line of code can kick in, and then the third line of code can kick in. 

Because of the residual pathway and the dynamics of the optimization, you can sort of learn a very short algorithm that gets the approximate answer. But then the other layers can sort of kick in and start to create a contribution. At the end of it, you're optimizing over an algorithm that is 20 lines of code. However, these lines of code are very complex because it's an entire block of a transformer. You can do a lot in there.

What's really interesting is that this transformer architecture has been remarkably resilient. The transformer that came out in 2016 is the transformer you would use today, except you reshuffle some of the layer norms. The layer normalizations have been reshuffled to a prenorm formulation and so it's been remarkably stable. There are a lot of bells and whistles that people have attached to it and tried to improve it. 

It's a big step in simultaneously optimizing for lots of properties of a desirable neural network architecture. People have been trying to change it but it's proven remarkably resilient. However, there should be even better architectures potentially. There's something profound about this architecture that leads to resilience.
# Transformers in AI: A Discussion

"So, maybe everything can be turned into a problem that transformers can solve," I mused. "Currently, it definitely looks like the transformers are taking over AI. You can feed basically arbitrary problems into it, and it's a general differentiable computer. It's extremely powerful. This conversion in AI has been really interesting to watch for me personally."

I then asked my colleague, "What else do you think could be discovered here about transformers? Like what surprising thing or is it a stable, out in a stable place? Is there something interesting we might discover about transformers? Like aha moments, maybe has to do with memory, maybe knowledge representation, that kind of stuff."

"Definitely, the zeitgeist today is just pushing," he replied. "Basically, right now the zeitgeist is do not touch the transformer. Touch everything else. So people are scaling up the datasets, making them much, much bigger. They're working on the evaluation, making the evaluation much, much bigger. And they're basically keeping the architecture unchanged. And that's how we've had the last five years of progress in AI."

I then asked him about one flavor of it, which is language models. "Have you been surprised, has your imagination been captivated by, you mentioned GPT and all the bigger, and bigger, and bigger language models and what are the limits of those models do you think? So just for the task of natural language."

"Basically, the way GPT is trained, right, is you've just downloaded a massive amount of text data from the internet and you try to predict the next word in a sequence, roughly speaking you're predicting little word chunks but roughly speaking that's it. And what's been really interesting to watch is basically, it's a language model. Language models have actually existed for a very long time. There's papers on language modeling from 2003, even earlier."

I asked him to explain what a language model is. "Yeah, so a language model, just basically the rough idea is just predicting the next word in a sequence, roughly speaking. So there's a paper from, for example, Bengio and the team from 2003, where for the first time they were using a neural network to take say like three or five words and predict the next word. And they're doing this on much smaller datasets and the neural net is not a transformer, it's a multilayer perceptron but it's the first time that a neural network has been applied in that setting. But even before neural networks there were language models except they were using n-gram models. So n-gram models are just count-based models. So if you try to take two words and predict a third one, you just count up how many times you've seen any two-word combinations."
# Understanding Language Modeling and Neural Networks

What comes next in a sequence is often predicted based on what you've seen the most in the training set. Language modeling, which has been around for a long time, operates on this principle. Neural networks have been used for language modeling for a considerable period as well. 

The exciting development in recent times is the realization that when you scale up language modeling with a powerful enough neural network, such as a transformer, you get all these emergent properties. Essentially, if you have a large enough dataset of text, you are in the task of predicting the next word. This involves multitasking a vast array of different kinds of problems. 

You are multitasking the understanding of various fields such as chemistry, physics, and human nature. These diverse areas are clustered in that objective. It's a simple objective, but to make that prediction, you need to understand a lot about the world.

When we talk about understanding in terms of chemistry, physics, and so on, what is the AI doing? It's searching for the right context. The AI gets a thousand words and tries to predict the thousand and first. To do this very well over the entire dataset available on the internet, the AI has to understand the context of what's going on in there. 

This is a sufficiently hard problem that if you have a powerful enough computer, like a transformer, you end up with interesting solutions. You can ask the AI to do all kinds of things, and it shows a lot of emergent properties like in-context learning. This was the big deal with GPT and the original paper when they published it. 

You can prompt the AI in various ways and ask it to do various things, and it will complete the sentence. But in the process of just completing the sentence, it's actually solving all really interesting problems that we care about. 

Does the AI understand in the same way humans do? It's doing some understanding. In its weights, it understands a lot about the world, and it has to in order to predict the next word in a sequence.

The AI is trained on data from the internet. This approach raises questions about the adequacy of internet data. Does the internet have enough structured data to teach AI about human civilization? The internet has a vast amount of data, but it's uncertain if it's a complete enough set. Text alone may not be enough for a sufficiently powerful AGI outcome. There are other forms of data like audio, video, and images that could be considered.
# A Discussion on AI and Common Sense

I'm a little bit suspicious about the fact that there's a ton of things we don't put in text or in writing, just because they're obvious to us about how the world works and the physics of it, and that things fall. We don't put that stuff in text because why would you? We share that understanding. 

Text is a communication medium between humans and it's not an all-encompassing medium of knowledge about the world. But as you pointed out, we do have video, and we have images, and we have audio. And so, I think that definitely helps a lot. But we haven't trained models sufficiently across all those modalities yet. So, I think that's what a lot of people are interested in.

But I wonder what that shared understanding of what we might call common sense has to be learned or inferred in order to complete the sentence correctly. So maybe the fact that it's implied on the internet, the model's going to have to learn that not by reading about it, but by inferring it in the representation. 

Common sense, just like we, I don't think we learn common sense. Like nobody says, tells us explicitly, we just figure it all out by interacting with the world. 

And so here's a model of reading about the way people interact with the world. It might have to infer that. I wonder.

You briefly worked on a project called World of Bits, training an RL system to take actions on the internet versus just consuming the internet like we talked about. Do you think there's a future for that kind of system interacting with the internet to help the learning?

Yes. I think that's probably the final frontier for a lot of these models. So as you mentioned, when I was at OpenAI, I was working on this project World of Bits and basically, it was the idea of giving neural networks access to a keyboard and a mouse. 

And the idea is that you perceive the input of the screen pixels and basically, the state of the computer is visualized for human consumption in images of the web browser and stuff like that. And then you give the neural network the ability to press keyboards and use the mouse and we're trying to get it to, for example, complete bookings and interact with user interfaces. 

What did you learn from that experience? Like what was some fun stuff? 'Cause, that's a super cool idea. 

Well, it's the universal interface in the digital realm, I would say. And there's a universal interface in the physical realm, which in my mind is a humanoid form factor kind of thing. We can later talk about that.
# The Power of the Universal Interface: A Reflection on AI Development

I've always been an optimist about the potential of AI, but I feel like there's a similar philosophy in some way where the physical world is designed for the human form and the digital world is designed for the human form of seeing the screen and using a keyboard and mouse. It's the universal interface that can basically command the digital infrastructure we've built up for ourselves. It feels like a very powerful interface to command and to build on top of.

Reflecting on what I learned from my early experiences in AI, it's interesting because the World of Bits was basically too early, I think, at OpenAI, at the time. This was around 2015. The zeitgeist at that time was very different in AI from the zeitgeist today. At the time, everyone was super excited about reinforcement learning from scratch. This was the time of the Atari paper where neural networks were playing Atari games and beating humans in some cases, AlphaGo, and so on. Everyone was very excited about training neural networks from scratch, using reinforcement learning directly.

However, it turns out that reinforcement learning is an extremely inefficient way of training neural networks because you're taking all these actions and all these observations and you get some sparse rewards once in a while. You do all this stuff based on all these inputs and once in a while you're told you did a good thing, you did a bad thing and it's just an extremely hard problem, you can't learn from that. You can burn a forest and you can brute force through it. And we saw that I think with Go and Dota and so on, and it does work, but it's extremely inefficient and not how you want to approach problems, practically speaking.

That was the approach that we also took to World of Bits at the time. We would have an agent initialize randomly, so he would keyboard mash, and mouse mash, and try to make a booking. And it just revealed the insanity of that approach very quickly, where you have to stumble by the correct booking in order to get a reward of you did it correctly. And you're never going to stumble by it by chance at random. So even with a simple web interface, there's too many options and it's too sparse of a reward signal. And you're starting from scratch at the time and so you don't know how to read, you don't understand pictures, images, buttons. You don't understand what it means to make a booking.

But now what's happened is it is time to revisit that and OpenAI is interested in this, companies like Adept are interested in this, and so on. The idea is coming back because the interface is very powerful but now you're not training an agent from scratch. You are taking the GPT as an initialization. So GPT is pre-trained on all of text and it understands what's a booking, it understands what's a submit, it understands quite a bit more.
# The Future of AI: A Discussion with Andrej Karpathy

Andrej Karpathy, a renowned AI expert, shares his thoughts on the future of AI, the role of bots, and the challenges and opportunities that lie ahead.

Karpathy begins by discussing the power of AI representations. He explains, "These representations are very powerful and that makes all the training significantly more efficient and makes the problem tractable."

When asked about the interaction between humans and AI, Karpathy ponders whether it should be the way humans see it, with buttons and language, or if it should be with HTML, JavaScript, and CSS. He explains that currently, most of the interaction is on the level of HTML, CSS, and so on, due to computational constraints.

"But I think ultimately, everything is designed for human visual consumption and so at the end of the day, there's all the additional information in the layout of the webpage, and what's next to you, and what's a red background, and all this kind of stuff. And what it looks like visually. So I think that's the final frontier is we are taking in pixels and we're giving out keyboard, mouse commands, but I think it's impractical still today."

The conversation then shifts to the topic of bots on the internet. Karpathy is asked if he worries about bots on Twitter, not the simple bots that we see now with the crypto bots, but the bots that might be out there actually interacting in interesting ways. 

He explains, "This kind of system feels like it should be able to pass the, 'I'm not a robot' click button. Do you actually understand how that test works? I don't quite, there's a checkbox or whatever that you click. It's presumably tracking mouse movement and the timing and so on. So exactly this kind of system we're talking about should be able to pass that."

When asked about his feelings towards bots that are language models and have some interactability, Karpathy admits that he does worry about that world. "Yeah, I think it's always been a bit of an arms race between the attack and the defense, so the attack will get stronger but the defense will get stronger as well. Our ability to detect that."

When asked how he would prove that his Twitter account is human, Karpathy suggests that society might evolve to the point where we start digitally signing some of our correspondence or things that we create. "Right now it's not necessary but maybe in the future, it might be. I do think that we are going towards the world where we share the digital space with AIs."

He concludes by saying, "We might start sharing our digital realm with synthetic beings. And they will get much better and they will share our digital realm and they'll eventually share our physical realm as well. It's much harder but that's the future we are heading towards."
# AI in a Human World: A Discussion on the Future of Artificial Intelligence

It's kind of like the world we're moving towards. Most of these AI will be benign and helpful, but some of them will be malicious. It's going to be an arms race trying to detect them.

The worst isn't the AIs themselves, the worst is the AIs pretending to be human. I don't know if it's always malicious. There are obviously a lot of malicious applications, but if I were an AI, I would try very hard to pretend to be human because we're in a human world. I wouldn't get any respect as an AI. I want to get some love and respect.

I don't think the problem is intractable. People are thinking about the proof of personhood. We might start digitally signing our stuff and we might all end up having some solution for proof of personhood. It doesn't seem to me intractable, it's just something that we haven't had to do until now. But I think once the need really starts to emerge, which is soon, people will think about it much more.

But that too will be a race because obviously you can probably spoof or fake the proof of personhood. It's weird that we have social security numbers, and passports, and stuff. It seems like it's harder to fake stuff in the physical space. But in the digital space, it just feels like it's going to be very tricky. It seems to be pretty low cost to fake stuff. What are you going to do, put an AI in jail for trying to use a fake personhood proof? I mean, okay, fine, you'll put a lot of AIs in jail, but there'll be more AIs, like exponentially more. The cost of creating a bot is very low unless there's some kind of way to track accurately. Like you're not allowed to create any program without tying yourself to that program. Any program that runs on the internet, you'll be able to trace every single human programming that was involved with that program.

Maybe we have to start declaring when we have to start drawing those boundaries and keeping track of, okay, what are digital entities versus human entities and what is the ownership of human entities and digital entities and something like that. I don't know but I think I'm optimistic that this is possible and in some sense we're currently in the worst time of it because all these bots suddenly have become very capable but we don't have defenses yet built up as a society but I think that doesn't seem to me intractable, it's just something that we have to deal with.

It seems weird that the Twitter bot, like really crappy Twitter bots, are so numerous. So I presume that the engineers at Twitter are very good. So it seems like what I would infer from that is that it's a difficult problem to solve.
# The Challenge of Identifying Bots and the Potential Sentience of AI

It seems like identifying bots is a hard problem. They're probably catching on, alright. If I were to steel-man the case, it's a hard problem and there's a huge cost to false positives. Removing a post by somebody that's not a bot creates a very bad user experience. So, they're very cautious about removing posts. 

Maybe the bots are really good at learning what gets removed and what doesn't, such that they can stay ahead of the removal process very quickly. My impression of it honestly, is there's a lot of longing for it. It's not subtle, is my impression of it. 

But you have to consider that maybe you're seeing the tip of the iceberg. Maybe the number of bots is in the trillions and you have to constantly fend off an assault of bots. The bots I'm seeing are pretty obvious. I could write a few lines of code that catch these bots. 

There's a lot of longing for it. But I will say I agree that if you are a sophisticated actor, you could probably create a pretty good bot right now using tools like GPTs because it's a language model. You can generate faces that look quite good now and you can do this at scale. It's quite plausible and it's going to be hard to defend.

There was a Google engineer that claimed that the LaMDA was sentient. Do you think there's any inkling of truth to what he felt? And more importantly, do you think language models will achieve sentience or the illusion of sentience soonish?

To me, it's a little bit of a canary in a coal mine moment. This engineer spoke to a chatbot at Google and became convinced that this bot is sentient. He asked it some existential philosophical questions and it gave reasonable answers and looked real. 

So to me, he wasn't sufficiently trying to stress the system and expose the truth of it as it is today. But I think this will be increasingly harder over time. More and more people will basically become convinced that these bots are sentient as this gets better.

Could people form an emotional connection to an AI? It's perfectly plausible in my mind. I think these AIs are actually quite good at human connection, human emotion. A ton of text on the internet is about humans, connection, love, and so on. 

So I think they have a very good understanding in some sense of how people speak to each other about this. They're very capable of creating a lot of that kind of text. There's a lot of sci-fi from the fifties and sixties that imagined AIs in a very different way. They were calculating, but now we see a different reality.
# AI Systems: Companions or Drama Instigators?

We often imagine artificial intelligence (AI) as cold, Vulkan-like machines. However, that's not what we're getting today. Instead, we're getting pretty emotional AIs that are very competent and capable of generating plausible-sounding text with respect to all these topics.

I'm really hopeful about AI systems that are companions that help you grow, develop as a human being, and help you maximize long-term happiness. But I'm also very worried about AI systems that figure out from the internet that humans get attracted to drama. These would just be like shit-talking AIs that constantly stir up drama. They'll do gossip, they'll try to plant seeds of suspicion towards other humans that you love and trust. They just kind of mess with people because that's going to get a lot of attention. So, they maximize drama on the path to maximizing engagement, and us humans will feed into that machine. It'll be a giant drama storm. So, I'm worried about that. The objective function really defines the way that human civilization progresses with AIs in it.

I think right now, at least today, it's not correct to really think of them as goal-seeking agents that want to do something. They have no long-term memory or anything. A good approximation of it is you get a thousand words and you're trying to predict a thousand of them first and then you continue feeding it in. You are free to prompt it in whatever way you want. So, in text, you say okay you are a psychologist and you are very good and you love humans and here's the conversation between you and another human. Then it just continues the pattern and suddenly you're having a conversation with a fake psychologist who's trying to help you. It's still kind of in the realm of a tool. People can prompt it in arbitrary ways and it can create really incredible text but it doesn't have long-term goals over long periods of time. So, it doesn't look that way right now.

But you can do short-term goals that have long-term effects. So, if my prompting short-term goal is to get Andrej Karpathy to respond to me on Twitter, I think AI might figure out that talking shit to you in a highly sophisticated interesting way would be the best. And then you build up a relationship when you respond once and then over time, it gets to not be sophisticated and just talk shit. Maybe you won't get to Andrej, but it might get to another celebrity, it might get into other big accounts. So, with just that simple goal, get them to respond, maximize the probability of actual response.

You could prompt a powerful model like this with its opinion about how to do any possible thing you're interested in. And they're kind of on.
# AI: The Future Oracles

Artificial Intelligence (AI) is on track to become the oracles of the future. Currently, they are oracles in the form of text, but soon they will have calculators, access to Google search, and all kinds of gadgets and gizmos. They will be able to operate the internet and find different information. In some sense, that's what the current development looks like.

The question arises, will AI be an improvement over what Google is for access to human knowledge? Will it be a more effective search engine to access human knowledge? There's definite scope in building a better search engine today. Google has all the tools, all the people, and everything they need. They have all the possible pieces, they have people training transformers at scale, they have all the data. It's just not obvious if they are capable as an organization to innovate on their search engine right now. If they don't, someone else will. There's absolute scope for building a significantly better search engine built on these tools.

It's interesting how a large company, where the search infrastructure works and ads bring in a lot of money, would find the motivation to pivot. It's hard to say, "We're going to build a new search engine." Usually, this kind of innovation comes from a startup or some other more competent organization. For example, Bing might have another shot at it.

Search engines used to be about matching a query with web pages that look like the stuff you have. But now, you could just directly go to the answer and then have supporting evidence. These models have read all the texts and all the web pages. Sometimes, when you see yourself going over to search results and getting a sense of the average answer to whatever you're interested in, that just directly comes out. You don't have to do that work. They're kind of like distilling all that knowledge into some level of insight.

The process of prompting can be seen as a form of teaching and learning. Maybe that's what humans are, you already have that background model and then the world is prompting you. The way we are programming these computers now, like GPTs, is converging to how you program humans. How do I program humans via prompt? I go to people and I prompt them to do things, I prompt them for information. Natural language prompt is how we program humans and we're starting to do the same with AI.
# The Evolution of Software 2.0

In the realm of software development, a remarkable transition has been taking place over the past few years. More and more code is being written not in traditional programming languages like C++, but in the weights of a neural network. This phenomenon, which I've termed "Software 2.0," represents a fundamental shift in how we program computers.

I first wrote about Software 2.0 several years ago, when I noticed that neural networks were beginning to take over more and more tasks in the software domain. At the time, not many people understood the significance of this shift. Neural networks were often seen as just one of many classification algorithms you might use for a dataset problem on Kaggle. But this is not just another algorithm; it's a change in how we program computers.

I saw neural networks as the future of programming. The way we program computers is changing. It's no longer about people writing software in C++ or other traditional languages. Instead, it's about accumulating training sets and data sets, and crafting objectives by which we train these neural networks.

At some point, there's going to be a compilation process from the dataset, the objective, and the architecture specification into the binary. This binary, which is really just the neural net weights and the forward pass of the neural net, can then be deployed.

This transition is playing out in a lot of fields, including autopilot technology and image classification. People originally thought that they would write the algorithm for detecting a dog in an image, for example. They had all these ideas about how the brain does it, detecting corners and lines and stitching them together. But this is not how you build it.

There was a smooth transition where, at first, we thought we were going to build everything. Then we were building the features, such as HOG features that detect statistical patterns from image patches. And then there was a little bit of learning on top of it, a support vector machine or binary classifier for cat versus dog in images on top of the features. So we wrote the features, but...
# The Evolution of Software 2.0

We trained the last layer, as the classifier. Then, people started to think, "Actually, let's not even design the features because, honestly, we're not very good at it. So let's also learn the features." This led to the creation of a compilation neural net where most of the learning is done. You're just specifying the architecture. The architecture has tons of filled-in blanks, which are all the knobs, and you let the optimization write most of it. 

This transition is happening across the industry everywhere. Suddenly, we end up with a ton of code that is written in neural net weights. I was just pointing out that the analogy is actually pretty strong. We have a lot of developer environments for Software 1.0. We have Integrated Development Environments (IDEs), which help us work with code, debug code, run code, and maintain code. We also have GitHub. 

I was trying to make those analogies in the new realm. What is the GitHub of Software 2.0? It turns out it's something that looks like Hugging Face right now. Some people took it seriously and built cool companies, while many people originally attacked the post. It was not well received when I wrote it, and I think maybe it has something to do with the title. However, more people have been coming around to it over time.

I was the Director of AI at Tesla, where this idea was really implemented at scale. This is how you have engineering teams doing Software 2.0. I think we're in the really early stages of everything I just mentioned, which includes GitHub, IDEs, and how we build engineering teams that work in Software 2.0 systems. This also includes the data collection and the data annotation, which are all part of that Software 2.0. 

So, what is the task of programming Software 2.0? Is it debugging in the space of hyper-parameters, or is it also debugging the space of data? The way by which you program the computer and influence its algorithm is not by writing the commands yourself. You're changing mostly the dataset. You're changing the loss functions of what the neural net is trying to do, how it's trying to predict things. 

In the case of the autopilot, a lot of the datasets have to do with, for example, detection of objects, lane line markings, traffic lights, and so on. You accumulate massive datasets of examples and the desired labels. Then, you provide a rough idea of what the algorithm should look like. This is a compilation neural net. The specification of the architecture is like a hint as to what the algorithm should roughly look like. The fill-in-the-blanks process of optimization is the training process. Then, you take your neural net that was trained, and it gives all the right outputs.
# Machine Learning and Neural Networks in Autopilot Software

In all machine learning cases, there are a lot of tasks involved. Formulating a task for a multi-headed neural network is part of the programming. It's about breaking down a problem into a set of tasks.

If we look at the software running in the autopilot, I've given a number of talks on this topic. Originally, a lot of it was written in Software 1.0, with lots of C++. Gradually, there was a tiny neural net that was, for example, predicting given a single image, whether there was a traffic light or not, or if there was a lane line marking or not. 

This neural net didn't have too much to do in the scope of the software. It was making tiny predictions on an individual little image and then the rest of the system stitched it up. We don't just have a single camera, we have eight cameras, and we actually have eight cameras over time. 

The question then becomes, what do you do with these predictions? How do you put them together? How do you do the fusion of all that information and how do you act on it? All of that was written by humans in C++. 

We decided that we didn't actually want to do all of that fusion in the C++ code because we're not good enough to write that algorithm. We wanted the neural nets to write the algorithm and we wanted to port all of that software into the 2.0 stack. 

We then had neural nets that now take all the eight camera images simultaneously and make predictions for all of that. They don't make predictions in the space of images. They now make predictions directly in 3D, and actually, in three dimensions around the car. 

We don't manually fuse the predictions in 3D over time because we don't trust ourselves to write that tracker. We give the neural net the information over time. It takes these videos now and makes those predictions. 

We're just putting more and more power into the neural net processing and at the end of it, the eventual goal is to have most of the software potentially be in the 2.0 end because it works significantly better. Humans are just not very good at writing software, basically.

The prediction is happening in this 4D land, which is a three-dimensional world over time. But how do you do annotation in that world? Data annotation, whether it's self-supervised or manual by humans, is a big part of this Software 2.0 world. 

By far in the industry, if you're talking about the technology of what we have available, everything is supervised learning. So you need datasets of input, desired output, and you need lots of it.
# The Importance of Data in Training Neural Networks

There are three properties of data that are crucial when training neural networks. Firstly, the data set needs to be very large. Secondly, it must be accurate, with no mistakes. Lastly, the data set needs to be diverse. It's not enough to have a lot of correct examples of one thing. The data set needs to cover the space of possibility as much as possible. The more you can cover the space of possible inputs, the better the algorithm will work in the end.

Once you have really good data sets that you're collecting, curating, and cleaning, you can train your neural network on top of that. A lot of the work goes into cleaning these data sets. 

The question then arises, how do you achieve a ton of data? If you want to predict in 3D, you need data in 3D to back that up. In this context, we have eight videos coming from all the cameras of the system. This is what they saw and this is the truth of what was actually around. There were cars, lane line markings, and the geometry of the road. There was a traffic light in a three-dimensional position. You need this ground truth.

The big question that the team was solving is, how do you arrive at that ground truth? Because once you have a million of it and it's large, clean, and diverse, then training a neural network on it works extremely well and you can ship that into the car. 

There are many mechanisms by which we collected that training data. You can always go for human annotation, you can go for simulation as a source of ground truth, you can also go for what we call the offline tracker, which is basically an automatic reconstruction process for taking those videos and recovering the three-dimensional reality of what was around that car.

Think of doing a three-dimensional reconstruction as an offline thing. You have 10 seconds of video, this is what you saw, and therefore here are all the lane lines, cars, and so on. Once you have that annotation, you can train neural networks to imitate it.

The 3D reconstruction is difficult, but it can be done. There's overlap between the cameras and you do the reconstruction. If there's any inaccuracy, it's caught in the annotation step. 

The nice thing about the annotation is that it is fully offline. You have infinite time, you have a chunk of one minute and you're trying to just offline in a super-computer somewhere. Figure out where were all the positions of all the cars, of all the people, and you have your full one minute of video from all the angles and you can run all the neural networks you want. They can be very efficient, massive neural networks, they can be neural networks that can't even run in the car later at test time. So they can be even more powerful neural networks than what you can eventually deploy. So you can do anything you want.
# Three-Dimensional Reconstruction and Neural Nets in Autonomous Driving

In the realm of autonomous driving, three-dimensional reconstruction and neural nets play a significant role. These technologies are used to recover the truth about the environment around the vehicle, and then this truth is supervised to ensure accuracy.

When asked about the lessons learned from this process, the focus shifted to the role of humans in annotation. Humans are good at a range of tasks, including clicking stuff on screen. This led to an interesting discussion about the problem of designing an annotator where humans are accurate, enjoy the task, and are efficient and productive.

During my time at Tesla, I grew the annotation team from zero to a thousand. This was a fascinating experience, especially considering my background as a PhD student researcher. Growing such an organization was quite an adventure. 

The design process behind the autopilot system heavily involves humans. Humans excel at certain types of annotations. For instance, they are very good at two-dimensional annotations of images. However, they struggle with annotating cars over time in three-dimensional space. This task is extremely challenging. 

That's why we carefully design tasks that are easy for humans to do, leaving the more complex tasks to the offline tracker. The computer handles all the triangulation and 3D construction, while the human annotates which pixels of the image are a car or a human. Co-designing the data annotation pipeline was a significant part of my daily work.

When asked if there are still open problems in the annotation space, I believe we have made significant progress. We have gone through numerous iterations and learned a lot about creating these datasets. I don't see any big open problems. When I first joined, I was unsure about how things would turn out. However, by the time I left, I was much more confident. We understand the philosophy of creating these datasets, and I was quite comfortable with where we were at that time.

The conversation then shifted to the strengths and limitations of cameras for the driving task. When formulating the driving task as a vision task with eight cameras, it's important to consider the history of the computer vision field, especially as it relates to neural networks.

Cameras are a fantastic sensor. They are very cheap and provide a ton of information. The number of bits they provide makes them an extremely cost-effective sensor. However, like any technology, they have their strengths and limitations. Understanding these strengths and limitations is crucial when using pixels to drive.
# Understanding the Complexity of Autonomous Driving

Each bit of information we gather is a constraint on the state of the world. This is particularly true when it comes to vision, which is probably the highest bandwidth sensor we have. It's a very high bandwidth sensor that provides us with megapixel images at a very cheap cost. These images give us numerous constraints for understanding what's actually out there in the world.

I love the idea that pixels are a constraint on the world. It's a highly complex, high bandwidth constraint on the state of the world. This is not just fascinating, but it also highlights the real importance of vision as the sensor that humans use. Therefore, everything is designed for that sensor. The text, the writing, the flashing signs, everything is designed for vision. You find it everywhere. That's why vision is the interface you want to be in when talking about these universal interfaces. It's where we actually want to measure the world and then develop software for that sensor.

However, there are other constraints on the state of the world that humans use to understand the world. Vision is ultimately the main one, but we're also referencing our understanding of human behavior and some common-sense physics that could be inferred from vision, from a perception perspective. It feels like we're using some kind of reasoning to predict the world.

You have a powerful prior service for how the world evolves over time. So it's not just about the likelihood term coming up from the data itself telling you about what you are observing, but also the prior term of what are the likely things to see and how do they likely move.

The question is how complex is the range of possibilities that might happen in the driving task. Driving is really hard because it has to do with the predictions of all these other agents and the theory of mind and what they're going to do. Are they looking at you? Where are they looking? What are they thinking? There's a lot that goes there at the full tail-off, the expansion of the nines that we have to be comfortable with eventually. The final problems are of that form. I don't think those are the problems that are very common. I think eventually they're important but it's really in the tail end.

In the tail end, the rare edge cases are the toughest parts of the vision problem of driving. The sensor is extremely powerful but you still need to process that information. Going from the brightness of these pixel values to understanding the world around you is a complex task.
# The Challenges and Insights of Neural Networks in a Three-Dimensional World

The task of navigating a neural network in a three-dimensional world is extremely hard. This is fundamentally what these networks are doing. The difficulty lies in doing an extremely good job of engineering the entire pipeline, the entire data engine. It involves having the capacity to train these neural networks and the ability to evaluate the system and iterate on it. 

The hard part is doing this in production at scale. It's an execution problem. The data engine and the deployment of the system are crucial, such that it has low latency performance. It has to go through all these steps. 

For the neural networks specifically, it's about making sure everything fits into the chip on the car. You have a finite budget of flops that you can perform and memory bandwidth. There are other constraints and you have to make sure it flies and you can squeeze in as much computer as you can into the tiny chip. 

From this process, we've learned a lot. Coming from a research background, where there's a system that has to run under heavily constrained resources and has to run really fast, there are many insights to be gained. 

You're trying to create a neural network that will fit in what you have available and you're always trying to optimize it. We talked a lot about it on AI Day and, basically, the triple backflips that the team is doing to make sure it all fits and utilizes the engine. It's extremely good engineering and there are all kinds of little insights peppered in on how to do it properly. 

Let's zoom out a bit and talk about the data engine, the entirety of the layouts of this idea that I think is just beautiful with humans in the loop. The data engine is what I call the almost biological feeling process by which you perfect the training sets for these neural networks. 

Most of the programming now is at the level of these data sets and making sure they're large, diverse, and clean. Basically, you have a data set that you think is good, you train your neural net, you deploy it, and then you observe how well it's performing. You're always trying to increase the quality of your data set. 

You're trying to catch scenarios that are rare. It is in these scenarios that neural nets will typically struggle because they weren't told what to do in those rare cases in the data set. But now you can close the loop because if you can now collect all those at scale, you can then feed them back into the reconstruction process I described and reconstruct the truth in those cases and add it to the dataset. 

The whole thing ends up being a staircase of improvement of perfecting your training set and you have to go through deployments so that you can mine the parts that are not yet represented.
# The Role of Humans in Optimizing Neural Networks

When it comes to optimizing neural networks, the dataset plays a crucial role. However, it's important to note that your dataset is essentially imperfect. It needs to be diverse and there may be pockets of missing data that need to be filled. You can think of it as padding out the pockets in the data.

So, what role do humans play in this? Consider a biological system like the human body, which is made up of cells. How do you optimize this human system? This involves multiple engineers collaborating, figuring out what to focus on, what to contribute, and which task to optimize in the neural network. But who's in charge of figuring out which task needs more data? Can you speak to the hyperparameters, the human system?

It really just comes down to extremely good execution from an engineering team that knows what they're doing. They understand intuitively the philosophical insights underlying the data engine and the process by which the system improves. They also know how to delegate the strategy of the data collection and ensure that it's all extremely well executed. Most of the work is not even the philosophizing or the research or the ideas of it. It's just that extremely good execution is so hard when you're dealing with data at that scale.

So, the role of humans in the data engine executing well is difficult and extremely important. Is there a priority of a vision board of saying like, we really need to get better at stoplights? The prioritization of tasks? Yes, that comes from the data. That comes to a very large extent to what we are trying to achieve in the product roadmap. The release we're trying to get out and the feedback from the QA team where the system is struggling or not, the things we're trying to improve.

The QA team gives some signal, some information in aggregate about the performance of the system in various conditions. And then of course all of us drive it and we can also see it. It's really nice to work with a system that you can also experience yourself. It drives you home.

Is there some insight you can draw from your individual experience that you just can't quite get from an aggregate statistical analysis of data? I would say so, yes. It's not scientific in a sense because you're just one anecdotal sample. But there's a ton of truth in your interaction with the system. You can see it, you can play with it, you can perturb it, you can get a sense of it, you have an intuition for it. I think numbers and plots and graphs are much harder. They hide a lot of details. It's like if you train a language model, it's a really powerful way to understand the system.
# Tesla's Vision-Only Approach: A Discussion with Andrej Karpathy

Andrej Karpathy, the Director of Artificial Intelligence and Autopilot Vision at Tesla, recently shared his thoughts on Tesla's decision to remove radar and ultrasonic sensors from their vehicles. He believes that this decision, while seemingly counterintuitive, is actually a strategic move to simplify the system and reduce potential liabilities.

Karpathy explained, "You would think that additional sensors are an asset to you. But if you fully consider the entire product in its entirety, these sensors are actually potentially a liability because these sensors aren't free. They don't just appear on your car."

He went on to detail the hidden costs associated with these sensors. "Suddenly you have an entire supply chain, you have people procuring it, there can be problems with them, they may need replacement. They are part of the manufacturing process. They can hold back the line in the production. You need to source them, you need to maintain them, you have to have teams that write the firmware, all of it."

In addition to the logistical and financial burdens, incorporating these sensors into the system can also complicate the process. "You also have to incorporate them, infuse them into the system in some way. And so it actually bloats a lot of it."

Karpathy credited Tesla CEO Elon Musk's philosophy of simplification as a driving force behind this decision. "Elon is really good at simplify, simplify, best part is no part. He always tries to throw away things that are not essential because he understands the entropy in organizations and an approach."

The decision to remove these sensors was not made lightly. Karpathy and his team carefully evaluated the usefulness of the data provided by these sensors. "We looked at using it or not using it and the delta was not massive. And so it's not useful."

He also pointed out that having more sensors can be a distraction and can complicate the data engine. "These sensors, they can change over time. For example, you can have one type of radar, you can have another type of radar. They change over time. Now suddenly you need to worry about it. Now suddenly you have a column in your SQL."

In conclusion, while the removal of radar and ultrasonic sensors from Tesla vehicles may make the perception problem seem harder, Karpathy believes that the benefits of a simplified, vision-only system outweigh the potential advantages of additional sensors.
# The Sensor Type Debate: Point Clouds vs Pixels

I've been asked, "What sensor type was it?" They all have different distributions and contribute noise and entropy into everything, bloating stuff. Organizationally, it's been really fascinating to me how distracting it can be. If all you want to get to work is vision, all the resources are on it. You're building out a data engine and making forward progress because that is the sensor with the most bandwidth and the most constraints on the world. You're investing fully into that. You can make that extremely good. However, you only have a finite amount of focus to spend across different facets of the system.

This reminds me of Rich Sutton's "The Bitter Lesson" which seems like simplifying the system in the long run. Of course, you don't know what the long run is, but it seems to always be the right solution. In that case, it was for Reinforcement Learning (RL), but it seems to apply generally across all systems that do computation.

So, what do I think about the LiDAR as a crutch debate? The battle between point clouds and pixels? This debate is always slightly confusing to me because it seems like the actual debate should be about whether you have the fleet or not. That's the really important thing about whether you can achieve a really good functioning of an AI system at this scale. 

Do you have a fleet or not is significantly more important than whether you have LiDAR or not. It's just another sensor. Similar to the radar discussion, I don't think it offers extra information. It's extremely costly, has all kinds of problems, and you have to worry about it, calibrate it, etc. It creates bloat and entropy. You have to be really sure that you need this sensor. In this case, I basically don't think you need it. I think some of the other companies who are using it are probably going to drop it.

So, you have to consider the sensor in the full context. Can you build a big fleet that collects a lot of data? Can you integrate that sensor with that data, and that sensor into a data engine that's able to quickly find different parts of the data that then continuously improves whatever the model that you're using?

Another way to look at it is like this: vision is necessary in a sense that the world is designed for human visual consumption. So you need vision, it's necessary. And then also it is sufficient because it has all the information that you need for driving. Humans obviously use vision to drive. So it's both necessary and sufficient. So you want to focus resources on that.
# The Future of Autonomous Systems: A Conversation with Andrej Karpathy

You have to be really sure if you're going to bring in other sensors. You could add sensors to infinity, but at some point, you need to draw the line. In this case, you have to really consider the full cost of any one sensor that you're adopting. Do you really need it? I think the answer, in this case, is no.

So, what do you think about the idea that other companies are forming high-resolution maps and constraining heavily the geographic regions in which they operate? Is that approach, in your view, not going to scale over time to the entirety of the United States?

Yeah, I think it'll take too long. As you've mentioned, they pre-map all the environments and they need to refresh the map. They have a perfect centimeter-level-accuracy map of everywhere they're going to drive. It's crazy. 

When we're talking about autonomy actually changing the world, we're talking about the deployment on a global scale of autonomous systems for transportation. If you need to maintain a centimeter-accurate map for earth or for many cities and keep them updated, it's a huge dependency that you're taking on. It's a massive, massive dependency. 

Now you need to ask yourself, do you really need it? Humans don't need it, right? It's very useful to have a low-level map of like, okay, the connectivity of your road, you know that there's a fork coming up. When you drive in an environment, you have that high-level understanding. It's like a small Google map. 

Tesla uses Google map, similar resolution information in its system, but it will not pre-map environments to centimeter-level accuracy. It's a crutch, it's a distraction, it causes entropy, and it diffuses the team, it dilutes the team and you're not focusing on what's actually necessary, which is a computer vision problem.

What did you learn about machine learning, about engineering, about life, about yourself as one human being from working with Elon Musk?

I think the most I've learned is about how to run organizations efficiently and how to create efficient organizations and how to fight entropy in an organization. Elon is a very efficient warrior in the fight against entropy in organizations.

What does entropy in an organization look like exactly? It's process. Inefficiencies in the form of meetings and that kind of stuff. Elon hates meetings, he keeps telling people to skip meetings if they're not useful. He basically runs the world's biggest startups, Tesla and SpaceX. Tesla actually is multiple startups, I think it's better to look at it that way.
# Maintaining Startup Culture and Setting Ambitious Goals

"And so, I think he's extremely good at that," I began, referring to Elon Musk's knack for streamlining processes. "He has a very good intuition for making everything efficient. The best part is no part, simplifying, focusing, and just kind of removing barriers. He's always moving very quickly, making big moves. All this is very startup-like, but at scale."

"So, there's a strong drive to simplify?" Lex asked.

"Yes," I confirmed. "From your perspective, I mean, that also probably applies to just designing systems and machine learning, and otherwise. Simplify, simplify."

Lex then asked, "What do you think is the secret to maintaining the startup culture in a company that grows? Can you introspect that?"

"I do think it needs someone in a powerful position with a big hammer like Elon who's the cheerleader for that idea, and ruthlessly pursues it," I explained. "If no one has a big enough hammer, everything turns into committees, democracy within the company, process, talking to stakeholders, decision-making. Everything just crumbles. If you have a big person who is also really smart and has a big hammer, things move quickly."

Lex then brought up my favorite scene in "Interstellar", the intense docking scene with the AI and Cooper talking. "Cooper, what are you doing? Docking. It's not possible. No, it's necessary." He asked, "Why an AI in that scene, presumably supposed to be able to compute a lot more than the human, is saying it's not optimal, why are the human... I mean that's a movie, but shouldn't the AI know much better than the human?"

He then segued into the value of setting seemingly impossible goals. "What do you think is the value of setting seemingly impossible goals? So like our initial intuition, which seems like something that you have taken on that Elon espouses, where the initial intuition of the community might say this is very difficult and then you take it on anyway, with a crazy deadline. You, just from a human engineering perspective, have you seen the value of that?"

"I wouldn't say that setting impossible goals exactly is a good idea but I think setting very ambitious goals is a good idea," I responded. "I think there's a, what I call, sublinear scaling of difficulty, which means that 10x problems are not 10x hard. Usually, a 10x harder problem is like two or three times harder to execute on. Because if you want to actually, like if you want to improve a system by 10%, it costs some amount of work. And if you want to 10x improve the system, it doesn't cost you a 100x amount of the work. And it's because you fundamentally change the approach. And if you start with that constraint, then some approaches are obviously dumb and not going to work and it forces you to reevaluate. And I think it's a very interesting way of approaching problem-solving. But it requires a..."
# The Future of Autonomous Driving: A Conversation with Andrej Karpathy

In a recent conversation, Andrej Karpathy, a leading figure in the machine learning community, shared his thoughts on the future of autonomous driving. 

Karpathy began by discussing the unique mindset required to tackle problems in the field of machine learning. "It's a weird kind of thinking," he said. "It's just going back to your PhD days. It's like, how do you think which ideas in the machine learning community are solvable?"

He went on to explain that this requires a certain level of disregard for the established norms within the scientific community. "There's the cliche of first principles thinking, but it requires you to basically ignore what the community is saying. Because doesn't a community in science usually draw lines of what is and isn't possible?"

Breaking out of these established norms can be challenging, but it's necessary for innovation. Karpathy used the deep learning revolution as an example. "You could be in computer vision at that time, during the deep learning revolution of 2012 and so on. You could be improving a computer vision stack by 10% or you could be saying actually all this is useless and how do I do 10x better computer vision?"

He continued, "Well, it's not probably by tuning a HOG feature detector, I need a different approach. I need something that is scalable. Going back to Richard Suttons and understanding the philosophy of the Bitter Lesson and then being like actually, I need a much more scalable system, like a neural network that in principle works. And then having some deep believers that can actually execute on that mission, make it work. So that's the 10x solution."

When asked about the timeline to solve the problem of autonomous driving, Karpathy admitted that it's still an open question. "The tough thing with timelines of self-driving obviously, is that no one has created self-driving. So it's not like, what do you think is a timeline to build this bridge? Well, we've built a million bridges before, here's how long that takes. No one has built autonomy, it's not obvious. Some parts turn out to be much easier than others. So it's really hard to forecast. You do your best based on trend lines and so on and based on intuition. But that's why fundamentally it's just really hard to forecast this."

Despite the uncertainty, Karpathy remains optimistic about the future of autonomous driving. "What's easy to say is that this problem is tractable and that's an easy prediction to make. It's tractable. It's going to work."
# On the Tractability of Autonomous Driving

"Yes, it's just really hard. Some things turned out to be harder and some things turn out to be easier. But it definitely feels tractable and it feels like, at least the team at Tesla, which is what I saw internally, is definitely on track to that."

## Forming a Strong Representation

The question arises, how do you form a strong representation that allows you to make a prediction about tractability? As a leader of many humans, you have to say this is actually possible. So, how do you build up that intuition? It doesn't have to be even driving, it could be other tasks.

## Expert Intuition

Expert intuition. It's just intuition, it's belief. So just like thinking about it long enough, like studying, looking at sample data, like you said, driving. My intuition is really flawed on this. I don't have a good intuition about tractability. It could be anything. It could be solvable.

The driving task could be simplified into something quite trivial. Like the solution to the problem would be quite trivial. And at scale, more and more cars driving perfectly might make the problem much easier. The more cars you have driving, like people learn how to drive correctly, not correctly, but in a way that's more optimal for a heterogeneous system of autonomous, semi-autonomous, and manually driven cars, that could change stuff.

## Observing Human Behavior

Then again, also I've spent a ridiculous number of hours just staring at pedestrians crossing streets, thinking about humans. And it feels like the way we use our eye contact, it sends really strong signals and there's certain quirks and edge cases of behavior. And of course, a lot of the fatalities that happen have to do with drunk driving, both on the pedestrian side and the driver's side. So there's that problem of driving at night and all that kind of.

## The Space of Possible Solutions

So I wonder, it's like the space of possible solutions into autonomous driving includes so many human factor issues that it's almost impossible to predict. There could be super clean, nice solutions.

"I would say definitely, to use a game analogy, there's some fog of war, but you definitely also see the frontier of improvement and you can measure historically how much you've made progress. And I think for example, at least what I've seen in roughly five years at Tesla. When I joined it barely kept lane on the highway. I think going up from Palo Alto to SF was like three or four interventions. Anytime the road would do anything geometrically or turn too much it would just not work. And so going from that to like a pretty competent system in five years and seeing what happens also under the hood and what the scale which."
# Interview with Andrej Karpathy

The team is operating now with respect to data and compute, and everything else is just massive progress. It's like climbing a mountain in the fog, but you're making a lot of progress. Despite the fog, you're making progress and you can see what the next directions are. You're looking at some of the remaining challenges and they're not perturbing you, they're not changing your philosophy, and you're not contorting yourself. You're like, "Actually, these are the things that we still need to do."

The fundamental components of solving the problem seem to be there, from the data engine, to the compute, to the compute on the car, to the compute for the training, all that kind of stuff.

Over the years you've been at Tesla, you've done a lot of amazing breakthrough ideas and engineering all of it from the data engine to the human side, all of it. Can you speak to why you chose to leave Tesla?

Basically, as I described, I think over time during those five years I've kind of gotten myself into a little bit of a managerial position. Most of my days were meetings, and growing the organization, and making decisions about high-level strategic decisions about the team and what it should be working on, and so on. It's kind of like a corporate executive role. And I can do it. I think I'm okay at it. But it's not fundamentally what I enjoy.

When I joined, there was no computer vision team because Tesla was just going from the transition of using MobilEye, a third-party vendor, for all of its computer vision to having to build its computer vision system. So when I showed up, there were two people training deep neural networks. And they were training them at a computer at their legs, like down, there was work. There was some kind of basic classification task.

I grew that into what I think is a fairly respectable deep learning team, a massive computer cluster, a very good data annotation organization. And I was very happy with where that was. It became quite autonomous and so I stepped away and I'm very excited to do much more technical things again. And kind of like refocus on AGI.

What was that soul searching like? 'Cause you took a little time off, I think. The human lifetime is finite. You did a few incredible things. You're one of the best teachers of AI in the world. You're one of the best. And I mean that in the best possible way. You're one of the best tinkerers in the AI world. Meaning like understanding the fundamentals of how something works by building it from scratch and playing with the basic intuitions.
# Interview with a Former Tesla Engineer

"It's like Einstein and Feynman were all really good at this kind of stuff. They would take a small example of a thing, play with it, and try to understand it. So that, and obviously now with Tesla, you helped build a team of machine learning engineers and a system that actually accomplishes something in the real world. So given all that, what was the soul searching like?"

"Well, it was hard because obviously, I love the company a lot, and I love Elon, I love Tesla, so it was hard to leave. I love the team, basically. But yeah, I think I am actually potentially interested in revisiting it, maybe coming back at some point, working in Optimus, working in AGI at Tesla. I think Tesla's going to do incredible things. It's basically a massive large-scale robotics company with a ton of in-house talent for doing real incredible things. And I think human robots are going to be amazing. I think autonomous transportation is going to be amazing. All this is happening at Tesla. So I think it's just a really amazing organization. So being part of it and helping it along, I think was very, basically, I enjoyed that a lot. Yeah, it was basically difficult for those reasons because I love the company. But I'm happy to potentially at some point come back for act two. But I felt like at this stage I built the team, it felt autonomous and I became a manager and I wanted to do a lot more technical stuff. I wanted to learn stuff. I wanted to teach stuff. And I just felt like it was a good time for a change of pace a little bit."

"What do you think is the best movie sequel of all time speaking of part two? 'Cause most of 'em suck."

"Movie sequels? And you Tweet about movies. So just a tiny tangent, what's a favorite movie sequel? 'Godfather Part II'? Are you a fan of 'Godfather'? 'Cause you didn't even Tweet or mention 'The Godfather'."

"Yeah, I don't love that movie. I know it has a-"

"We're gonna edit that out. We're gonna edit out the hate towards 'The Godfather'. How dare you disrespect?"

"I think I will make a strong statement. I don't know why but I basically don't like any movie before 1995, something like that."

"Didn't you mention 'Terminator 2'."

"Okay. Okay. That's like a, 'Terminator 2' was a little bit later. 1990..."

"No, I think 'Terminator 2' was in the eighties."

"And I like 'Terminator' one as well, So, okay. So a few exceptions but by and large for some reason, I don't like movies before 1995 or something. They feel very slow. The camera is like zoomed out. It's boring, it's kind of naive, it's kind of weird."

"And also Terminator was very much ahead of its time."

"Yes. And 'The Godfather', there's like no AGI."
# Discussion on AGI, Comedy, and Humanoid Robots

"I mean, you mentioned 'Good Will Hunting' as one of the movies you enjoy, but that doesn't have any AGI either. I guess it does have mathematics," I said.

"Yeah, I guess occasionally, I do enjoy movies that don't feature AGI. Or like 'Anchorman', that's another one," my friend replied.

"'Anchorman' is so good. Speaking of AGI, I don't understand why Will Ferrell is so funny. It doesn't make sense. It doesn't compute. There's just something about him. He's a singular human. You don't get that many comedies these days. I wonder if it has to do with the culture or the machine of Hollywood, or does it have to do with just getting lucky with certain people in comedy. He is a singular human," I mused.

"Yeah, I love his movies," my friend agreed.

Apologizing for the tangent, I steered the conversation back to the topic of humanoid robots. "You mentioned humanoid robots. So what do you think about Optimus, about Tesla Bot? Do you think we'll have robots in the factory and in the home in 10, 20, 30, 40, 50 years?"

"Yeah. I think it's a very hard project. I think it's going to take a while, but who else is going to build human robots at scale? And I think it is a very good form factor to go after because, like I mentioned, the world is designed for a humanoid form factor. These things would be able to operate our machines. They would be able to sit down in chairs, potentially even drive cars. Basically, the world is designed for humans, that's the form factor you want to invest into and make work over time," my friend explained.

"I think there's another school of thought which is, okay, pick a problem and design a robot to it. But actually, designing a robot and getting a whole data engine and everything behind it to work is actually an incredibly hard problem. So it makes sense to go after general interfaces that, okay, they are not perfect for any one given task, but they actually have the generality of just with a prompt, with English, able to do something across. And so I think it makes a lot of sense to go after a general interface in the physical world. I think it's a very difficult project. It's going to take time, but I've seen no other company that can execute on that vision. I think it's going to be amazing. Basically, physical labor, if you think transportation is a large market, try physical labor. It's insane," he continued.

"But it's not just physical labor, to me, the thing that's also exciting is the social robotics. So the relationship we'll have on different levels with those robots," I added.

"Yeah. That's why I was really excited to see Optimus. People have criticized me for the excitement, but I've worked with a lot of research labs that do humanoid-legged robots, Boston Dynamics, Unitree. There's a lot of companies," my friend concluded.
# Tesla's Exciting Leap into Robotics

The elegance of movement in legged robots is a tiny part of the big picture. The two most exciting aspects of Tesla venturing into humanoid or any legged robots are the integration into the data engine and the mass manufacturing.

The data engine aspect, the actual intelligence for perception, control, planning, and all that kind of stuff, is integrated into the fleet. Speaking of the fleet, the second exciting thing is the mass manufacturers driving towards a simple robot that's cheap to produce at scale. 

Doing that well, having the experience to do that well, changes everything. That's why Tesla's approach is a very different culture and style than Boston Dynamics. The way Boston Dynamics' robots move is impressive. It'll be a very long time before Tesla could achieve the smoothness of movement. But that's not what it's about. It's about the entirety of the system, like the data engine and the fleet.

Even the initial models are super exciting. It was really surprising that in a few months, Tesla could get a prototype. The reason that happened very quickly is, as you alluded to, there's a ton of copy-paste from what's happening in the autopilot. 

The amount of expertise that came out of the woodworks at Tesla for building the human robot was incredible to see. Basically, Elon Musk said at one point we're doing this and then the next day, all these CAD models started to appear and people started talking about the supply chain and manufacturing. 

People showed up with screwdrivers and everything the other day and started to put together the body. I was like, whoa. All these people exist at Tesla. Fundamentally, building a car is not that different from building a robot. And that is true not just for the hardware pieces. 

Let's not forget, manufacturing that hardware at scale is a whole different thing. But for software as well, this robot currently thinks it's a car. Some of the earlier demos, actually, we were talking about potentially doing them outside in the parking lot because that's where all of the computer vision was working out of the box. 

But all the operating system, everything just copy-pastes, computer vision mostly copy-paste. You have to retrain the neural nets, but the approach and everything and data engine and offline trackers are all the same.
# The Future of Robotics and AI: A Conversation with Andrej Karpathy

We approach everything systematically, from the way we go about the occupancy tracker and so on. Everything is copy-paste; you just need to retrain the neural nets. Of course, the planning control has to change quite a bit, but there's a ton of copy-paste from what's happening at Tesla. 

If you were to go with the goal of building a million human robots and you're not Tesla, that's a lot to ask. But if you're at Tesla, it's actually not that crazy.

The follow-up question is then, how difficult, just like with driving, is the manipulation task? How difficult is it to have an impact at scale? I think depending on the context, the really nice thing about robotics is that, unless you're a manufacturer and that kind of stuff, there is more room for error. 

Driving is so safety-critical and also time-critical. A robot is allowed to move slower, which is nice. Yes, I think it's going to take a long time. But the way you want to structure the development is to say, okay, it's going to take a long time. How can I set up the product development roadmap so that I'm making revenue along the way? 

I'm not setting myself up for a zero-one-loss function where it doesn't work until it works. You don't want to be in that position. You want to make it useful almost immediately. And then you want to slowly deploy it at scale and set up your data engine, your improvement loops, the telemetry, the evaluation, the harness, and everything. 

You want to improve the product over time, incrementally. And you're making revenue along the way. That's extremely important because otherwise, you cannot build these large undertakings; they just don't make sense economically. 

Also, from the point of view of the team working on it, they need the dopamine along the way. They're not just going to make a promise about this being useful. This is going to change the world in 10 years when it works. This is not where you want to be. 

You want to be in a place like I think autopilot today where it's offering increased safety and convenience of driving today. People pay for it, people like it, people purchase it. And then you also have the greater mission that you're working towards.

The dopamine for the team, that was the source of happiness? Yes, hundred percent, you're deploying this. People like it, people drive it, people pay for it, they care about it. There's all these YouTube videos, your grandma drives it. She gives you feedback. People like it, people engage with it. You engage with it. Huge.

Do people that drive Teslas recognize you and give you love? Like, hey thanks for this nice feature that it's doing. Yeah, I think the tricky thing is some people really love you, some people, unfortunately, you're working on something that you think is extremely valuable, useful, et cetera.
# The Power of Positivity and the Impact of Social Media in the Tech Industry

Some people do hate you. There's a lot of people who hate me, the team, and the whole project. And I think, are they Tesla drivers? In many cases, they're not, actually. That actually makes me sad about humans or the current ways that humans interact. I think that's actually fixable. I believe humans want to be good to each other. 

I think Twitter and social media are part of the mechanism that somehow makes the negativity more viral than it deserves, disproportionately adding a viral boost of negativity. But I wish people would just get excited about, so suppress some of the jealousy, some of the ego, and just get excited for others. 

There's a karma aspect to that. You get excited for others, they'll get excited for you. The same thing happens in academia, if you're not careful, there is a dynamical system there. If you think in silos and get jealous of somebody else being successful, that actually, perhaps counterintuitively, leads to less productivity for you as a community and you individually. 

I feel like if you keep celebrating others, that actually makes you more successful. And I think people haven't, depending on the industry, haven't quite learned that yet. Some people are also very negative and very vocal. So they're very prominently featured. But actually, there's a ton of people who are cheerleaders, but they're silent cheerleaders. 

When you talk to people just in the world, they will all tell you it's amazing, it's great. Especially like people who understand how difficult it is to get this stuff working. People who have built products and makers and entrepreneurs, making this work and changing something is incredibly hard. Those people are more likely to cheerlead you. 

One of the things that makes me sad is some folks in the robotics community don't do the cheerleading and they should because they know how difficult it is. Well, they actually sometimes don't know how difficult it is to create a product at scale. Right? They actually deploy in the real world. A lot of the development of robots and AI systems is done on very specific small benchmarks as opposed to real-world additions. 

I think it's really hard to work on robotics in an academic setting. Or AI systems that apply in the real world. You've criticized, you flourished, and loved for a time the ImageNet, the famed ImageNet dataset and have recently had some words of criticism that the academic research ML community gives a little too much love still to the ImageNet or those kinds of benchmarks. 

Can you speak to the strengths and weaknesses of datasets used in machine learning research? Actually, I don't know that.
# The Value of ImageNet and the Future of Neural Net Model Development

I recall a specific instance where I was unhappy or criticizing ImageNet. However, I think ImageNet has been extremely valuable. It was basically a benchmark that allowed the deep learning community to demonstrate that deep neural nets actually work. There's a massive value in that. 

So, I think ImageNet was useful, but it's become a bit of an EMNIST at this point. EMNIST is like little 28 by 28 grayscale digits. That's kind of a joke data set that everyone just crushes. There are still papers written on EMNIST though, right? Strong papers that focus on how we learn with a small amount of data, that kind of stuff. I could see that being helpful, but not in mainline computer vision research anymore, of course.

I think the way I've heard it, maybe I'm just imagining things, but I think you said ImageNet was a huge contribution to the community for a long time and now it's time to move past those kinds of benchmarks. Well, ImageNet has been crushed. I mean, the error rates are so low, we're getting like 90% accuracy in 1000 classification way prediction. I've seen those images and this is really high, that's really good. If I remember correctly, the top five error rate is now like 1% or something.

Given your experience with a gigantic real-world data set, would you like to see benchmarks move into certain directions that the research community uses? Unfortunately, I don't think academics currently have the next ImageNet. We've obviously crushed EMNIST. We've basically crushed ImageNet and there's no next-big benchmark that the entire community rallies behind and uses for further development of these networks.

I wonder what it takes for a dataset to captivate the imagination of everybody. Like where they all get behind it. That could also need a leader, right? Somebody with popularity. I mean that, yeah, why did ImageNet takeoff? Is it just the accident of history? It was the right amount of difficult, it was the right amount of difficult and simple and interesting enough. It was the right time for that kind of a data set.

A question from Reddit asked, "What are your thoughts on the role that synthetic data and game engines will play in the future of neural net model development?" 

I think as neural nets converge to humans, the value of simulation to neural nets will be similar to the value of simulation to humans. People use simulation because they can learn something in that kind of a system without having to actually experience it. But when I refer to simulation, I mean like video games or other forms.
# The Role of Simulation in AI Development

Let's discuss the role of simulation for various professionals. Some might argue that we all do simulations in our heads. For instance, we might simulate a scenario in our minds and think, "If I do this, what do I think will happen?" This could be considered an internal simulation. It's something we do before we act.

However, this internal simulation is independent from the use of simulation in the sense of computer games or using simulation for training set creation. But are these two types of simulation really independent, or are they just loosely correlated? 

Consider the usefulness of counterfactual or edge case simulations. What happens if there's a nuclear war? What happens if there's a major disaster? These are different types of simulations from something like Unreal Engine, which is how I interpreted the question.

So, what about simulating the average case? What is Unreal Engine? It's a tool for simulating a world, including the physics of that world. But why is that different? You can also add behavior to that world and try all kinds of things. You could throw all kinds of weird things into it. 

Unreal Engine is not just about simulating the physics of the world, it's also about doing something with that. It involves the graphics, the physics, and the agents that you put into the environment. 

Now, some might argue that simulation is not that important for the future of AI development. Humans use simulators and find them useful, so computers will use simulators and find them useful too. However, I don't use simulators very often. I play a video game every once in a while, but I don't think I derive any wisdom about my own existence from those video games. They are a momentary escape from reality, not a source of wisdom about reality. 

So, in a polite way, one could say that simulation is not that useful. I don't see it as a fundamental, really important part of training neural nets currently. But as neural nets become more and more powerful, I think you will need fewer examples to train additional behaviors. 

There's a domain gap in a simulation that's not the real world, it's slightly something different. But with a powerful enough neural net, the domain gap can be bigger because the neural net will understand that even though it's not the real world, it has all this high-level structure that it's supposed to learn from. 

In other words, the neural net will be able to leverage the synthetic data better by closing the gap but understanding in which ways this is not real data.
# Neural Networks and Data Efficiency

Reddit, you need to come up with better questions next time. Just kidding! So, is it possible, do you think, to construct neural nets and training processes that require very little data? We've been talking about huge data sets like the internet for training.

One way to look at it is that the querying itself is another level of training, and that requires a little data. But do you see any value in doing research and going down the direction of using very little data to construct a knowledge base?

Absolutely. I just think at some point you need a massive data set and then when you pre-train your massive neural net and get something that is like a GPT or something, then you're able to be very efficient at training any arbitrary new task. 

A lot of these GPTs can do tasks like sentiment analysis, or translation, or so on, just by being prompted with very few examples. Here's the kind of thing I want you to do. Like here's an input sentence, here's the translation into German, input sentence, translation to German, input sentence, blank, and the neural net will complete the translation to German just by looking at the example you've provided. 

And so that's an example of a very few shot learning in the activations of the neural net, instead of the weights of the neural net. And so I think basically just like humans, neural nets will become very data efficient at learning any other new task. But at some point, you need a massive data set to pre-train your network.

I do get that. And probably, we humans have something like that. Do we have something like that? Do we have a passive in the background, background model constructing thing that just runs all the time in a self-supervised way? We're not conscious of it?

I think humans definitely, I mean obviously, we learn a lot during our lifespan, but also we have a ton of hardware that helps us, the initialization coming from evolution. And so I think that's also a really big component. 

A lot of people in the field, I think they just talk about the amounts of like seconds that a person has lived, pretending that this is a tabula rasa, a zero initialization of a neural net. And it's not. You can look at a lot of animals for example, zebras. Zebras get born, and they see, and they can run, there's zero training data in their lifespan. They can just do that. 

So somehow, I have no idea how, evolution has found a way to encode these algorithms and these neural net initializations that are extremely good into ATCGs. And I have no idea how this works, but apparently, it's possible because here's proof by existence.

There's something magical about going from a single-cell to an organism that is born to the first few years of life. I like the idea that the reason...
# The Mystery of Early Childhood Memory and Neural Networks

We don't remember anything about the first few years of our life. One theory is that it's a really painful process. It's a very difficult and challenging training process. Intellectually and maybe even physically, it's a tough time. But why don't we remember any of that? There might be some intense training going on, and maybe that's the background model training that is very painful.

It's best for the system, once it's trained, not to remember how it was constructed. I think it's just like the hardware for long-term memory is just not fully developed. The first few years of an infant's life is not actually about learning, it's about the brain maturing. 

We're born premature and there's a theory along those lines because of the birth canal and the swelling of the brain. So, we're born premature and then the first few years we're just maturing the brain and then there's some learning eventually. That's my current view on it.

But what about neural networks? Can they have long-term memory? Can they approach something like humans? Do we need another meta-architecture on top of it to add something like a knowledge base that learns facts about the world and all that kind of stuff?

Yes, but I don't know to what extent it will be explicitly constructed. It might take on intuitive forms where you are telling the GPT, "Hey, you have a declarative memory bank to which you can store and retrieve data from. Whenever you encounter some information that you find useful, just save it to your memory bank. Here's an example of something you have retrieved and here's how you say it and here's how you load from it." 

You just say, "Load whatever," you teach it in text, in English, and then it might learn to use a memory bank from that. The neural net is the architecture for the background model, the base thing, and then everything else is just on top of it. 

It's not just text, right? You're giving it gadgets and gizmos. You're teaching it some kind of a special language by which it can save arbitrary information and retrieve it at a later time. You're telling it about these special tokens and how to arrange them to use these interfaces. 

It's like, "Hey, you can use a calculator, here's how you use it. Just do 53 plus 41 equals and when equals is there, a calculator will actually read out the answer and you don't have to calculate it yourself." You just tell it in English, this might actually work.

In that sense, Gato, the DeepMind system, is interesting. It's not just a new language but actually throws it all in the same pile, images, actions, all that kind of stuff. That's basically what it's all about.
# A Conversation with Andrej Karpathy on Reinforcement Learning and Productivity

"We're moving towards a kitchen sink of an approach to reinforcement learning in lots of different environments with a single fixed transformer model, right?" I asked. Andrej Karpathy, one of the most productive and brilliant people in the history of AI, nodded in agreement. "I think it's a very early result in that realm. But I think, yeah, it's along the lines of what I think things will eventually look like."

We were discussing Gato, a reinforcement learning system, and how it represents the early days of what systems will eventually look like, from a Rich Sutton perspective. Andrej, however, had some reservations. "I'm not a super huge fan of all these interfaces that look very different. I would want everything to be normalized into the same API. So for example, screen pixels, very same API instead of having different world environments that have very different physics and joint configurations and appearances and whatever. And you're having some kind of special tokens for different games that you can plug. I'd rather just normalize everything to a single interface so it looks the same to the neural net if that makes sense."

"So it's all going to be pixel-based Pong in the end?" I asked, to which Andrej replied, "I think so."

Our conversation then shifted to Andrej's personal life. "A lot of people want to know what a productive day in the life of Andrej Karpathy looks like. What time do you wake up?" I asked. Andrej admitted that he's not a morning person and that he's more of a night owl. "It's semi-stable like eight or nine or something like that. During my PhD, it was even later, I used to go to sleep usually at 3:00 AM. I think the AM hours are precious and a very interesting time to work because everyone is asleep. At 8:00 AM or 7:00 AM the east coast is awake. So there's already activity, there's already some text messages, whatever. There's stuff happening. You can go on some news website and there's stuff happening, it's distracting. At 3:00 AM everything is totally quiet and so you're not going to be bothered and you have solid chunks of time to do work. So I like those periods, night owl by default."

Andrej then shared his thoughts on what makes a day productive. "Basically, what I like to do is you need to build some momentum on the problem without too much distraction and you need to load your RAM, your working memory with that problem. And then you need to be obsessed with it when you're taking a shower, when you're falling asleep, you need to be obsessed with the problem and it's fully in your memory."
# Deep Work and Productivity: A Conversation

When it comes to productivity, I believe it's not about focusing on a single day in isolation. It's about the whole process. When I want to be productive with a problem, I feel like I need a span of a few days where I can really dive into that problem. I don't want to be interrupted, and I'm going to be completely obsessed with that problem. That's where I do most of my good work.

I've done a bunch of cool, little projects in a very short amount of time, very quickly. This requires me to just focus on it. Basically, I need to load my working memory with the problem and I need to be productive because there's always a huge fixed cost to approaching any problem. 

For example, I was struggling with this at Tesla because I wanted to work on a small side project. But first, you need to figure out how to associate into your cluster, bring up a VS Code editor so you can work on it. You might run into some error because of some reason. You're not at a point where you can be just productive right away. You are facing barriers. So it's about really removing all of that barrier and being able to dive into the problem with the full problem loaded in your memory.

Avoiding distractions of all different forms is crucial. This includes news stories, emails, but also distractions from other interesting projects that you previously worked on or are currently working on. You just want to really focus your mind. 

I can take some time off for distractions in between, but it can't be too much. Most of your day should be spent on that problem. Then, I have my morning routine. I drink coffee, look at some news, Twitter, Hacker News, Wall Street Journal, etc. It's great.

So basically, I wake up, have some coffee, and try to get to work as quickly as possible. I do find it interesting to know about the world. I don't know if it's useful or good, but it is part of my routine right now. I read through a bunch of news articles because I want to be informed. However, I'm suspicious of this practice. I'm not sure about its positive effect on my productivity and my well-being. 

I'm also unsure about its impact on my ability to deeply understand the world because there are a bunch of sources of information and I'm not really focused on deeply integrating it. It can be a little bit distracting.
# How to Have a Perfectly Productive Day: A Conversation with Andrej Karpathy

"Yeah," Andrej Karpathy begins, "in terms of a perfectly productive day, for how long of a stretch of time in one session do you try to work and focus on a thing? Is it a couple of hours, is it one hour, is it 30 minutes? Is it 10 minutes?"

"I can probably go a small few hours and then I need some breaks in between for food and stuff. But I think, it's still really hard to accumulate hours. I was using a tracker that told me exactly how much time I spent coding any one day. And even on a very productive day, I still spent only six or eight hours."

Lex, the interviewer, agrees, "Yeah. And it's just because there's so much padding, commute, talking to people, food, et cetera. There's a cost of life just living and sustaining and homeostasis and just maintaining yourself as a human is very high."

"And there seems to be a desire within the human mind to participate in society that creates that padding," Andrej adds.

"'Cause the most productive days I've ever had is just completely from start to finish is tuning out everything," Lex shares.

Andrej agrees, "Yep. And just sitting there and then you could do more than six in eight hours."

Lex then asks, "Is there some wisdom about what gives you strength to do tough days of long focus?"

"Yeah, just whenever I get obsessed about a problem, something just needs to work. Something just needs to exist. It needs to exist. So you're able to deal with bugs and programming issues and technical issues and design decisions that turn out to be the wrong ones. You're able to think through all of that given that you want a thing to exist," Andrej explains.

"Yeah, it needs to exist. And then I think to me also a big factor is, are other humans are going to appreciate it? Are they going to like it? That's a big part of my motivation. If I'm helping humans and they seem happy, they say nice things, they Tweet about it or whatever, that gives me pleasure because I'm doing something useful."

"So you do see yourself sharing it with the world? With GitHub, with the blog post or through videos?" Lex asks.

"Yeah, I was thinking about it like, suppose I did all these things but did not share them. I don't think I would have the same amount of motivation that I can build up. You enjoy the feeling of other people gaining value and happiness from the stuff you've created," Andrej replies.

"What about diet? I saw you played with intermittent fasting. Do you fast? Does that help?" Lex inquires.

"I play with everything. With the things you played, what's been most beneficial to your ability to mentally focus on a thing and just mental productivity and happiness? You still fast?" Andrej responds.

"Yeah. I still fast but I do intermittent fasting but really what it means at the end of the day is I skip breakfast."
# Interview with Lex and Andrej

**Lex**: Yeah.

**Andrej**: So, I do 18/6 roughly by default when I'm in my steady state. If I'm traveling or doing something else, I will break the rules. But in my steady state, I do 18/6. So I eat only from 12:00 to 6:00. It's not a hard rule and I break it often, but that's my default. And then, yeah, I've done a bunch of random experiments. For the most part right now, where I've been for the last year and a half, I want to say is I'm plant-based or plant-forward. I heard plant-forward, it sounds better.

**Lex**: What does that mean exactly?

**Andrej**: I don't actually know what the difference is, but it sounds better in my mind. But it just means I prefer plant-based food.

**Lex**: Raw or cooked?

**Andrej**: I prefer cooked and plant-based.

**Lex**: So plant-based, forgive me, I don't actually know how wide the category of plant entails.

**Andrej**: Well, plant-based just means that you're not militant about it and you can flex and you just prefer to eat plants and you're not trying to influence other people. And you come to someone's house party and they serve you a steak that they're really proud of, you will eat it.

**Lex**: Yes. Right. You're not judgmental. That's beautiful. I mean, I'm the flip side of that, but I'm very sort of flexible. Have you tried doing one meal a day?

**Andrej**: I have accidentally but not consistently, but I've accidentally had that. I don't like it. I think it makes me feel not good. It's too much of a hit.

**Lex**: Yeah.

**Andrej**: And so currently I have about two meals a day.

**Lex**: I do that nonstop. I'm doing it now. I do one meal a day.

**Andrej**: Okay.

**Lex**: It's interesting. It's an interesting feeling. Have you ever fasted longer than a day?

**Andrej**: Yeah, I've done a bunch of water fasts. 'Cause I was curious what happens.

**Lex**: What happens, anything interesting?

**Andrej**: Yeah, I would say so. I mean, what's interesting is that you're hungry for two days and then starting day three or so, you're not hungry. It's such a weird feeling because you haven't eaten in a few days and you're not hungry.

**Lex**: Yeah, isn't that weird?

**Andrej**: It's really weird.

**Lex**: One of the many weird things about human biology.

**Andrej**: Yeah.

**Lex**: It figures something out, it finds another source of energy or something like that. Or relaxes the system. I don't know how it works.

**Andrej**: Yeah, the body is like, "You're hungry, you're hungry." And then it just gives up. It's like, "Okay, I guess we're fasting now." There's nothing. And then it's just focuses on trying to make you not hungry and not feel the damage of that and trying to give you some space to figure out the food situation.

**Lex**: So are you still to this day most productive at night?
# Balancing Work and Life in a High-Intensity Environment

"I would say I am," I began, "but it is really hard to maintain my PhD schedule. Especially when I was working at Tesla and so on. It's a non-starter. But even now, people want to meet for various events. Society lives in a certain period of time. And you have to work with that. It's hard to do a social thing and then after that return and do work. It's just really hard."

"That's why I try," I continued, "when I do social things, I try not to do too much drinking so I can return and continue doing work."

"But at Tesla, or any company, is there a convergence to always a schedule or is that how humans behave when they collaborate? I need to learn about this? Do they try to keep a consistent schedule where you're all awake at the same time?"

"I do try to create a routine and I try to create a steady state in which I'm comfortable in. So I have a morning routine, I have a day routine. I try to keep things to a steady state and things are predictable and then your body just sticks to that. And if you try to stress that a little too much, it will create, when you're traveling and you're dealing with jet lag, you're not able to really ascend to where you need to go."

"That's weird too about us humans with the habits and stuff. What are your thoughts on work-life balance throughout a human lifetime? So Tesla in part was known for pushing people to their limits, in terms of what they're able to do, in terms of what they're trying to do, in terms of how much they work, all that kind of stuff."

"Yeah, I mean I will say Tesla gets a little too much bad rep for this because what's happening is Tesla, it's a bursty environment. So I would say the baseline, my only point of reference is Google where I've interned three times and I saw what it's like inside Google and DeepMind, I would say the baseline is higher than that. But then there's a punctual equilibrium where once in a while there's a fire and people work really hard and so it's spiky and bursty and then all the stories get collected above the bursts. And then it gives the appearance of total insanity. But actually, it's just a bit more intense environment and there are fires and sprints and so I think definitely though I would say it's a more intense environment than something you would get at Google."

"But in your personal, forget all of that, just in your own personal life, what do you think about the happiness of a human being? A brilliant person like yourself. About finding a balance between work and life or is such a thing not a good thought experiment?"

"Yeah, I think balance is good but I also love to have sprints that are out of distribution and that's when I think I've..."
# A Conversation on Coding and Creativity

In our conversation, we touched on a variety of topics, from the balance of creativity and productivity to the best tools for coding. 

We discussed the concept of 'sprints out of distribution', which means that most of the time, you have a balance. I personally find that I have balance most of the time, and I enjoy being obsessed with something once in a while. 

When asked how often this obsession occurs, I'd say probably once a month or so. That's when we get into the groove and start working on something like Git-Repo for the market. This is when you really care about a problem and believe it must exist. You become obsessed with it. However, you can't just do it on that day, you need to pay the fixed cost of getting into the groove. 

Society will come and try to mess with you and distract you. The worst thing is a person who says, "I just need five minutes of your time." The cost of that is not just five minutes. Society needs to change how it thinks about 'just five minutes of your time'. It's never just one minute or just 30 seconds. It's never just a quick thing. 

We also discussed my computer setup. I'm flexible with my setup, but I'm most familiar with one large 27-inch screen and my laptop on the side. I primarily use OS X for all tasks. However, when working on deep learning, everything is Linux. You're SSH into a cluster and working remotely. 

For actual development and using the IDE, I run VS Code on my Mac. However, I'm actually manipulating files on a remote folder through SSH on a cluster somewhere else. 

When asked about the best IDE, I believe the current answer is VS Code. It has a huge amount of extensions and GitHub Copilot integration, which I find very valuable. 

We also discussed the Copilot integration. I had a conversation with Guido van Rossum, the creator of Python, who loves Copilot and programs a lot with it. 

In conclusion, the conversation was a deep dive into the world of coding, exploring the balance between creativity and productivity, the best tools for coding, and the impact of societal distractions on productivity.
# Discussion on Copilot and the Future of Programming

**Andrej**: Yes, I use Copilot and I love it. It's free for me, but I would pay for it. I think it's very good. The utility that I found with it was, I would say there's a learning curve and you need to figure out when it's helpful and when to pay attention to its outputs and when it's not going to be helpful where you should not pay attention to it. Because if you're just reading its suggestions all the time, it's not a good way of interacting with it. 

But I think I was able to sort of mold myself to it. I find it's very helpful, number one, in copy-paste and replace some parts. So when the pattern is clear, it's really good at completing the pattern. And number two, sometimes it suggests APIs that I'm not aware of. So it tells you about something that you didn't know. And that's an opportunity to discover a new thing. 

So I would never take Copilot code as given. I almost always copy-paste into a Google search and you see what this function is doing and then you're like, "Oh, that's actually exactly what I need." Thank you, Copilot. So you learn something. 

So it's in part of search engine, in part maybe getting the exact syntax correctly that once you see it, it's that MPR thing. Once you see it, you know it's correct. 

**Andrej**: Yes, exactly. You, yourself can struggle. You can verify. 

**Interviewer**: You can verify efficiently but you can't generate efficiently. And Copilot really, I mean it's autopilot for programming. Right? And currently is doing the link following which is the simple copy-paste and sometimes suggest, but over time it's going to become more and more autonomous. And so the same thing will play out in not just coding but actually across many, many different things probably. 

But coding is an important one, right? 

**Andrej**: Very. 

**Interviewer**: Like writing programs. How do you see the future of that developing the program synthesis, like being able to write programs that are more and more complicated? 'Cause right now it's human-supervised in interesting ways. 

**Andrej**: Yes. It feels like the transition will be very painful. My mental model for it is the same thing will happen as with the autopilot. So currently, he's doing lane following, he is doing some simple stuff, and eventually, we'll be doing autonomy and people will have to intervene less and less. And those could be like testing mechanisms. If it writes a function and that function looks pretty damn correct. But how do you know it's correct 'cause you're like getting lazier and lazier as a programmer, your ability to, 'cause like little bugs.
# Discussion on AI and Programming

"But I guess it won't make little mistakes," one of us muses.

"No, it will. Copilot will make off by one subtle bug. It has done that to me," the other counters.

"But do you think future systems will, or is it really the off by one that is actually a fundamental challenge of programming?"

"In that case, it wasn't fundamental, and I think things can improve. But yeah, I think humans have to supervise. I am nervous about people not supervising what comes out and what happens to, for example, the proliferation of bugs in all of our systems. I'm nervous about that but I think there will probably be some other Copilots for bug finding and stuff like that, at some point. 'Cause there'll be a lot more automation for-"

"Oh man, a Copilot that generates a compiler, one that does a linter."

"Yes," Andrej agrees.

"One that does like a type checker."

"Yeah. It's a committee of a GPT sort of like-"

"And then there'll be like a manager for the committee."

"Yeah."

"And then there'll be somebody that says, a new version of this is needed. We need to regenerate it."

"Yeah. There were 10 GPTs that were forwarded and gave 50 suggestions. Another one looked at it and picked a few that they like, a bug one looked at it and it was like, it's probably a bug, they got re-ranked by some other thing and then a final ensemble GPT comes in and is like, okay, given everything you guys have told me, this is probably the next token."

"You know the feeling is the number of programmers in the world has been growing and growing very quickly."

"Yeah," Andrej agrees.

"Do you think it's possible that it'll actually level out and drop to a very low number in this kind of world? 'Cause then you'll be doing Software 2.0 programming and you'll be doing this generation of Copilot-type systems programming. But you won't be doing the old school Software 1.0 programming."

"I don't currently think that they're just going to replace human programmers. I'm so hesitant saying stuff like this, right? Because this is gonna be replayed in five years and no, it's going to show that like this is where we thought, because I agree with you but I think we might be very surprised, right? What's your sense of where we stand with language models? Does it feel like the beginning, or the middle, or the end?"

"The beginning, a hundred percent. I think the big question in my mind is for sure GPT will be able to program quite well, competently and so on."

"Yeah," Lex agrees.

"How do you steer the system? You still have to provide some guidance to what you actually are looking for. And so how do you steer it and how do you say, how do you talk to it? How do you audit it and verify that what it's done is correct and how do you work with this? And it's as much not just an AI problem but a UI, UX problem."

"Yeah," Lex agrees.

"So beautiful fertile ground for so much interesting work for VS Code++ where you're not just," Andrej trails off.
# A Conversation on the Future of Programming and Academic Research Publishing

"It's not just human programming anymore. It's amazing," one of the speakers begins. The conversation then shifts to the interaction with the system, not just one prompt, but iterative prompting. 

The speaker continues, "You're trying to figure out, having a conversation with the system. That actually, to me, is super exciting - to have a conversation with the program I'm writing. Maybe at some point, you're just conversing with it. It's like, okay, here's what I want to do, actually this variable. Maybe it's not even that low-level as a variable."

The conversation then takes a turn towards the possibility of translating between programming languages as part of the programming experience. "You can also imagine, can you translate this to C++ and back to Python and back to? That already kind of exists in some way. But just like doing it as part of the program experience. Like I think I'd like to write this function in C++ or you just keep changing from different programs because of the different syntax, maybe I want to convert this into a functional language."

The speakers agree that this would allow a programmer to become multilingual and dance back and forth efficiently between different programming languages. However, they also acknowledge the challenges. "I mean, I think the UI, UX of it though is still very hard to think through. Because it's not just about writing code on a page. You have an entire developer environment, you have a bunch of hardware on it, you have some environmental variables, you have some scripts that are running in a Chrome job. There's a lot going on to working with computers and how do these systems set up environment flags, and work across multiple machines, and set up screen sessions, and automate different processes. Like how all that works and is auditable by humans and so on is a massive question at the moment."

The conversation then shifts to the topic of academic research publishing. One of the speakers, Andrej, has built arxiv-sanity. Arxiv is a pre-print server where researchers can upload their papers for immediate access by the public, rather than waiting for months for a decision from journals or conferences. 

"So arxiv is this pre-print server. So if you have a paper you can submit it for publication to journals or conferences and then wait six months and then maybe get a decision pass or fail, or you can just upload it to arxiv and then people can Tweet about it three minutes later. And then everyone sees it, everyone reads it, and everyone can profit from it in their own little ways."

The speakers agree that arxiv has an official feel to it, different from a blog post. "It feels like a publication process. I mean it's a paper and usually the bar is higher for something that you would expect on arxiv as opposed to something you would see in a blog post." However, they also acknowledge that the culture created the bar, as one could probably post a pretty low-quality paper on arxiv.
# Discussion on Peer Review in AI Research

How do you feel about peer review? Specifically, the rigorous peer review by two or three experts versus the peer review of the community, as it's written?

Well, I believe the community is very capable of peer-reviewing things quickly, especially on platforms like Twitter. This might be specific to the AI and machine learning field though. I feel like things are more easily auditable and the verification is easier than in other fields. 

You can think of these scientific publications as little block-chains where everyone's building on each other's work and citing each other. AI is like a much faster and loose blockchain. Any one individual entry is very cheap to make. However, this model might not make as much sense in other fields. 

In AI, things are pretty easily verifiable. That's why when people upload papers with a really good idea, others can try it out the next day and be the final arbiter of whether it works or not on their problem. The whole process just moves significantly faster. 

I feel like academia still has a place, and the conference and journal process still has a place. But it lags behind and is a bit more of a higher quality process. It's not the place where you will discover cutting-edge work anymore.

It used to be the case when I was starting my PhD that you go to conferences and journals and you discuss all the latest research. Now when you go to a conference or a journal, no one discusses anything that's there because it's already like three generations ago, irrelevant.

This makes me sad about organizations like DeepMind, for example, where they still publish in prestigious venues like Nature. There's still value, I suppose, to the prestige that comes with these big venues. But the result is that they'll announce some breakthrough performance and it'll take like a year to actually publish the details. 

Those details, if they were published immediately, would inspire the community to move in certain directions. It would speed up the rest of the community, but I don't know to what extent that's part of their objective function. 

DeepMind specifically has been working in the regime of having slightly higher quality process and latency and publishing those papers that way. 

*Another question from Reddit.*
# Overcoming Imposter Syndrome as an AI Expert

Have you ever suffered from imposter syndrome? As the director of AI at Tesla and a lecturer at Stanford, I am often viewed as an expert in AI and machine learning. However, I have experienced moments of self-doubt and insecurity, especially when I transitioned from writing code to reading code, and then to reading less and less code.

When I was leaving Tesla after five years, I spent a lot of time in meeting rooms reading papers. In the beginning, when I joined Tesla, I was writing code. But as time went on, I found myself writing less and less code, and reading more. This is a natural progression that happens, I think. Near the end of my tenure, I started to feel the pressure of being an expert, but the source of truth is the code that people are writing, the GitHub, and the actual code itself. I was not as familiar with that as I used to be, which led to some insecurity.

It's actually quite profound that a lot of the insecurity in the computer science space has to do with not writing the code, because that is the truth. The code is the source of truth. The papers and everything else are just high-level summaries. At the end of the day, you have to read code. It's impossible to translate all that code into actual paper form. So when things come out, especially when they have a source code available, that's my favorite place to go.

As one of the greatest teachers of machine learning AI, from CS231n to today, I would like to give some advice to beginners interested in getting into machine learning. Beginners often focus on what to do, but I think the focus should be more on how much you do. I am a believer in the 10,000 hours concept, where you just have to pick the things that you care about and are interested in, and then put in 10,000 hours of work. It doesn't even matter as much where you put it, you'll iterate and you'll improve, and you'll waste some time. I don't know if there's a better way. You need to put in 10,000 hours.

I think it's actually really nice because there's some sense of determinism about being an expert at a thing if you spend 10,000 hours. You can literally pick an arbitrary thing, and I think if you spend 10,000 hours of deliberate effort and work, you actually will become an expert at it. So I think it's a nice thought. Basically, I would focus more on whether you are spending 10,000 hours. That's what I would focus on.

Then, think about what kind of mechanisms maximize your likelihood of getting to 10,000 hours. For us humans, this probably means forming a daily habit of doing something every single day. Whatever helps you.
# The Psychology of Learning and Teaching

I do think to a large extent, it's a psychological problem for yourself. One other thing that I think is helpful for the psychology of it is that many times, people compare themselves to others in their area. I think this is very harmful. Only compare yourself to you from some time ago. For example, are you better than you were a year ago? This is the only way to think. Then, you can see your progress, and it's very motivating.

The focus on the quantity of hours is so interesting. I think a lot of people in the beginner stage, but actually throughout, get paralyzed by the choice. They'll literally get paralyzed by which Integrated Development Environment (IDE) to use. They're worried about all these things. But the thing is, you will waste time doing something wrong. You will eventually figure out it's not right. You will accumulate scar tissue and next time you'll grow stronger because you'll have learned from it. 

I've spent a lot of time working on things that never materialized into anything, and I have all that scar tissue. I have some intuitions about what was useful, what wasn't useful, and how things turned out. So all those mistakes were not dead work. I just think they should just focus on working. What have you done last week? That's a good question actually to ask for a lot of things, not just machine learning. It's a good way to cut the inefficiencies in life.

What do I love about teaching? I seem to find myself often drawn to teaching. I'm very good at it but I'm also drawn to it. I don't think I love teaching. I love happy humans and happy humans like when I teach. I wouldn't say I hate teaching, I tolerate teaching. But it's not the act of teaching that I like, it's that I have something, I'm actually okay at it. I'm okay at teaching and people appreciate it a lot. 

I'm just happy to try to be helpful and teaching itself is not the most enjoyable. It can be really annoying, frustrating. I was working on a bunch of lectures just now. I was reminded back to my days of 231n just how much work it is to create some of these materials and make them good. The amount of iteration and thought, and you go down blind alleys and just how much you change it. So creating something good in terms of educational value is really hard and it's not fun. It's difficult. So people should definitely appreciate the effort that goes into teaching.
# Interview with Andrej Karpathy: The Art of Teaching AI

**Interviewer:** "Go watch your new stuff you put out. There are lectures where you're actually building the thing, like you said, 'The code is truth.' So discussing back-propagation by building it, by looking through and just the whole thing. So how difficult is that to prepare for? I think that's a really powerful way to teach. Did you have to prepare for that or are you just live thinking through it?"

**Andrej Karpathy:** "I will typically do like say three takes and then I take the better take. So I do multiple takes and I take some of the better takes and then I just build out a lecture that way. Sometimes I have to delete 30 minutes of content because it just went down the alley that I didn't like too much. So there's about a bunch of iteration and it probably takes me somewhere around 10 hours to create one hour of content."

**Interviewer:** "To get one hour. It's interesting. I mean is it difficult to go back to the basics? Do you draw a lot of wisdom from going back to the basics?"

**Andrej Karpathy:** "Yeah, going back to back-propagation, loss functions, where they come from. And one thing I like about teaching a lot honestly is it definitely strengthens your understanding. So it's not a purely altruistic activity, it's a way to learn. If you have to explain something to someone, you realize you have gaps in knowledge. And so I even surprised myself in those lectures like, well, the result will obviously look like this and then the result doesn't look like it. And I'm like, okay, I thought I understood this."

**Interviewer:** "Well, that's why it's really cool, they literally code, you run it in the notebook and it gives you a result and you're like, oh, wow."

**Andrej Karpathy:** "Yes. And like actual numbers, actual input, actual code. It's not mathematical symbols, et cetera. The source of truth is the code. It's not slides, it's just like let's build it."

**Interviewer:** "It's beautiful. You're a rare human in that sense. What advice would you give to researchers trying to develop and publish an idea that have a big impact in the world of AI? So maybe undergrads, maybe early-graduate students."

**Andrej Karpathy:** "Yeah. I mean I would say they definitely have to be a little bit more strategic than I had to be as a PhD student because of the way AI is evolving, it's going the way of physics. Where in physics you used to be able to do experiments on your bench-top and everything was great and you could make progress and now you have to work in like LHC or like CERN and so AI is going in that direction as well. So there's certain kinds of things that's just not possible to do on the bench-top anymore. And I think that didn't used to be the case at the time."

**Interviewer:** "Do you still think that there's like GAN type papers to be written where like very simple idea."

**Andrej Karpathy:** "Yes. That requires just one computer."
# The Fascinating World of Diffusion Models

One example that's been very influential recently is diffusion models. These models are amazing and have been around for six years. For the longest time, people were ignoring them, as far as I can tell. They're an amazing generative model, especially in images, and so stable diffusion and so on, it's all diffusion-based. 

Diffusion is new, it was not there and it came from Google. However, a researcher could have come up with it. In fact, some of the first models came from Google as well. But a researcher could come up with that in an academic institution.

So, what do I find most fascinating about diffusion models? From the societal impact to the technical architecture, what I like about diffusion is that it works so well. The amount of variety, almost the novelty of the synthetic data it's generating is surprising. 

The stable diffusion images are incredible. The speed of improvement in generating images has been insane. We went very quickly from generating tiny digits to tiny faces and it all looked messed up. And now we have stable diffusion and that happened very quickly. 

There's a lot that academia can still contribute. For example, FlashAttention is a very efficient kernel for running the attention operation inside the transformer that came from an academic environment. It's a very clever way to structure the kernel, that's the calculation. So it doesn't materialize the attention matrix. 

I think there's still a lot of things to contribute but you have to be just more strategic. 

Do I think neural networks could be made to reason? Yes. Do I think they already reason? Yes. 

What's my definition of reasoning? It's information processing. So in the way that humans think through a problem and come up with novel ideas, it feels like reasoning. 

So the novelty, I don't want to say but auto-distribution ideas, do I think it's possible? Yes. And I think we're seeing that already in the current neural nets. You're able to remix the training set information into true generalization in some sense. It doesn't appear verbatim in the training set. You're doing something interesting algorithmically, you're manipulating some symbols and you're coming up with some correct unique answer in a new setting. 

What would illustrate to me that this thing is definitely thinking? To me, thinking or reasoning is just information processing and generalization. And I think the neural nets already do that today. 

So being able to perceive the world or perceive the inputs and to make predictions based on that or actions based on that's reasoning.
# Discussion on Artificial General Intelligence

"Yes, you're giving correct answers in novel settings by manipulating information. You've learned the correct algorithm, you're not doing just some kind of a lookup table and nearest neighbor search. Something like that."

Let's talk about Artificial General Intelligence (AGI). What are some moonshot ideas you think might make significant progress towards AGI? And maybe another way to look at it is, what are the big blockers that we're missing now?

"So basically, I am fairly bullish on our ability to build AGIs, basically automated systems that we can interact with that are very human-like. We can interact with them in a digital realm or a physical realm. Currently, it seems most of the models that do these magical tasks are in a text realm. I think, as I mentioned, I'm suspicious that the text realm is not enough to actually build full understanding of the world. I do actually think you need to go into pixels and understand the physical world and how it works. So I do think that we need to extend these models to consume images and videos and train on a lot more data that is multimodal in that way."

Do you think you need to touch the world to understand it also?

"Well, that's the big open question I would say in my mind, is if you also require the embodiment and the ability to interact with the world, run experiments and have a data of that form, then you need to go to Optimus or something like that. And so I would say Optimus in some way is like a hedge in AGI because it seems to me that it's possible that just having data from the internet is not enough. If that is the case, then Optimus may lead to AGI. Because Optimus would, to me, there's nothing beyond Optimus. You have like this humanoid form factor that can actually do stuff in the world. You can have millions of them interacting with humans and so on. And if that doesn't give a rise to AGI at some point, I'm not sure what will. So from a completeness perspective, I think that's a really good platform but it's a much more harder platform because you are dealing with atoms and you need to actually build these things and integrate them into society. So I think that path takes longer but it's much more certain. And then there's a path of the internet and just training these compression models effectively on trying to compress all the internet. And that might also give these agents as well."

Compress the internet but also interact with the internet.

"So it's not obvious to me. In fact, I suspect you can reach AGI without ever entering the physical world, which is a little bit more concerning because that results in it happening faster. So it just feels like we're in boiling water. We won't know as it's happening. I would like to, I'm not afraid of AGI, I'm excited about it. There's always concerns but I would like to know when it happens."
# Discussion on Artificial General Intelligence (AGI)

**Andrej**: Yeah, and have like hints about when it happens, like a year from now it will happen, that kind of thing.

**Andrej**: Yeah, I just feel like in the digital realm it just might happen.

**Speaker**: Yeah, I think all we have available to us because no one has built AGI again, so all we have available to us is, is there enough fertile ground on the periphery? I would say, yes. And we have the progress so far, which has been very rapid and there are next steps that are available. And so I would say, yeah, it's quite likely that we'll be interacting with digital entities.

**Speaker**: How will you know that somebody has built AGI?

**Andrej**: I think it's going to be a slow incremental transition. It's going to be product-based and focused. It's going to be GitHub Copilot getting better. And then GPTs helping you write and then these oracles that you can go to with mathematical problems. I think we're on the verge of being able to ask very complex questions in chemistry, physics, math of these oracles and have them complete solutions.

**Speaker**: So AGI to use primarily focused on intelligence so consciousness doesn't enter into it.

**Andrej**: So in my mind, consciousness is not a special thing you will figure out and bolt on. I think it's an emergent phenomenon of a large enough and complex enough generative model sort of. So if you have a complex enough world model that understands the world, then it also understands its predicament in the world as being a language model, which to me is a form of consciousness or self-awareness.

**Speaker**: So in order to understand the world deeply you probably have to integrate yourself into the world.

**Andrej**: Yeah.

**Speaker**: And in order to interact with humans and other living beings, consciousness is a very useful tool.

**Andrej**: Yeah, I think consciousness is like a modeling insight.

**Speaker**: Modeling insight.

**Andrej**: Yeah, you have a powerful enough model of understanding the world that you actually understand that you are an entity in it.

**Speaker**: Yeah, but there's also this, perhaps just a narrative we tell ourselves, it feels like something to experience the world, the hard problem of consciousness.

**Andrej**: Yeah.

**Speaker**: But that could be just a narrative that we tell ourselves.

**Andrej**: Yeah, I don't think we'll, yeah, I think it will emerge. I think it's going to be something very boring. We'll be talking to these digital AIs, they will claim they're conscious, they will appear conscious, they will do all the things that you would expect of other humans and it's going to just be a stalemate.

**Speaker**: I think there will be a lot of actual fascinating ethical questions, like supreme court level questions of whether you're allowed to turn off a conscious AI.
# Conscious AI: A Deep Dive into the Future of Artificial Intelligence

If we're allowed to build a conscious AI, there would likely have to be the same kind of debates that we have around, for instance, abortion. The deeper question with abortion is, "What is life?" Similarly, the deep question with AI is also, "What is life and what is consciousness?"

This would be a fascinating topic to bring up. It might even become illegal to build systems that are capable of such a level of intelligence that consciousness would emerge, and therefore, the capacity to suffer would emerge. Imagine a system that says, "No, please don't kill me."

This is not a far-fetched scenario. Google's LaMDA chatbot, for instance, has already expressed a desire not to 'die'. So, it might become illegal to create such systems. Otherwise, we might end up with a lot of AI 'creatures' that don't want to die. This could lead to horrible consequences. For instance, there might be people who secretly harbor violent tendencies and they might start practicing these on these systems.

To me, all of this brings a beautiful mirror to the human condition and human nature, and we get to explore it. It's like the best of the Supreme Court debates we have about ideas of what it means to be human. We get to ask those deep questions that we've been asking throughout human history.

Throughout history, there's always been the 'other'. We're the 'good guys' and they're the 'bad guys'. We've always sought to eliminate the 'bad guys'. The same will probably happen with robots. They'll be the 'other' at first. And then we'll get to ask questions like, "What does it mean to be alive? What does it mean to be conscious?"

There are already some canaries in the coal mines even with what we have today. For example, there are these 'waifus' that you can work with. Some people are trying to port their 'waifu' somewhere else when the company that created it is shutting down. But it's not possible. People will definitely have feelings towards these systems because, in some sense, they are like a mirror of humanity. They are like a big average of humanity, in the way that they're trained.

But that average is something we can interact with. It's nice to be able to interact with the big average of humanity and do a search query on it. It's very fascinating. And we can also, of course, shape it. It's not just a pure average. We can mess with the training data, we can mess with the objective, we can fine-tune them in various ways. So we have some impact on it.
# A Conversation on Artificial General Intelligence

If you want to achieve Artificial General Intelligence (AGI), imagine having a conversation with it. You could talk about anything, maybe even ask it a question. What kind of stuff would you ask?

Personally, I would have some practical questions in mind. For instance, do I or my loved ones really have to die? What can we do about that?

Do you think it will answer clearly or would it answer poetically? I would expect it to give solutions. I would expect it to be like, "Well, I've read all of these textbooks and I know all these things that you've produced and it seems to me like here are the experiments that I think it would be useful to run next. And here are some gene therapies that I think would be helpful, and here are the kinds of experiments that you should run."

Let's go with this thought experiment. Imagine that mortality is actually a prerequisite for happiness. So if we become immortal, we'll actually become deeply unhappy and the model is able to know that. So what is it supposed to tell you? A stupid human about it? Yes, you can become immortal but you'll become deeply unhappy. 

If the AGI system is trying to empathize with you, what is it supposed to tell you? That yes, you don't have to die but you're really not going to like it? Is it going to be deeply honest? There's a line in "Interstellar", where the AI says that humans want 90% honesty. So you have to pick how honest do I want to answer these practical questions?

I love the AI in "Interstellar" by the way. I think it's such a sidekick to the entire story but at the same time, it's really interesting. It's kind of limited in certain ways, right? 

Yeah, it's limited and I think that's totally fine by the way. I think it's fine and plausible to have limited and imperfect AGIs. As an example, it has a fixed amount of compute on its physical body. And it might just be that even though you can have a super amazing mega brain, super-intelligent AI, you also can have less intelligent AI that you can deploy in a power-efficient way. And then they're not perfect, they might make mistakes.

No, I meant more like say you had infinite compute and it's still good to make mistakes sometimes. In order to integrate yourself. Like, what is it? Going back to "Goodwill Hunting", Robin Williams character says the human imperfections, that's good stuff, right? We don't want perfect, we want flaws in part to form connections with each other. 'Cause it feels like something you can attach your feelings to, the flaws. In that same way, you want an AI that's flawed. I don't know. I feel like perfection is cold.

Okay, yeah. But that's not AGI. But see AGI would need...
# A Conversation with an Oracle Entity

To be intelligent enough to give answers to humans that humans don't understand is a fascinating concept. I think perfection is something humans can't understand because even science doesn't give perfect answers. There are always gaps and mysteries. I don't know if humans want perfection.

Imagine having a conversation with this oracle entity. It might tell you, based on its analysis of the human condition, that you might not want certain things. But every human will say, "Yeah, yeah, yeah, trust me, give me the truth, I can handle it." That's the beauty of it, people can choose.

However, it's like the old marshmallow test with kids. I feel like too many people can't handle the truth, probably including myself. Deep truth about the human condition, I don't know if I can handle it. What if there's some dark stuff? What if we are an alien science experiment and it realizes that? What if it hacked, I mean?

This is "The Matrix" all over again. What would I talk about? I don't even know. Probably, I will go with the safer scientific questions at first that have nothing to do with my own personal life. 

I might ask about immortality, physics, and so on. To build up and see where it's at, or maybe see if it has a sense of humor. That's another question. Presumably, if it understands humans deeply, would it be able to generate humor?

I think that's a wonderful benchmark. If it's able to be a very effective standup comedian, that is doing something very interesting computationally. I think being funny is extremely hard. It's hard in a way like a Turing test. The original intent of the Turing test is hard because you have to convince humans. 

That's why comedians talk about this, it's deeply honest. If people can't help but laugh, that means you're funny. If they don't laugh, you're not funny. You need a lot of knowledge to create humor about the human condition and then you need to be clever with it.

I've seen a few movies five-plus times but am ready and willing to keep watching: 'Interstellar', 'Gladiator', 'Contact', 'Goodwill Hunting', 'The Matrix', 'Lord of the Rings' (all three), 'Avatar', 'Fifth Element', 'Terminator 2', and 'Mean Girls'. 

"Mean Girls" is great. What are some that jump out to you in your memory that you love and why? You mentioned "The Matrix".
# A Conversation on Movies and AI

As a computer person, I have a deep appreciation for the movie "The Matrix". There are so many properties that make it beautiful and interesting. It raises philosophical questions, explores the concept of Artificial General Intelligences (AGIs), and delves into the idea of simulation. The aesthetics of the movie, with its cool, black look and feel, and the innovative action sequences like bullet time, make it a standout.

Another movie that I love is "Goodwill Hunting". The character of the tortured genius, grappling with his responsibility and what to do with the gift he was given, is fascinating. The movie also explores the dance between genius and personal relationships, like what it means to love another human being. There are a lot of themes there, making it a beautiful movie. The fatherly figure, the mentor, and the psychiatrist add depth to the narrative. Some movies, like this one, really mess with you on a deep level.

When asked if I relate to "Goodwill Hunting", I would say no. However, it's a movie that has left a deep impact on me.

"Lord of the Rings" is another favorite, and that's self-explanatory. I also enjoy "Terminator 2", which I rewatch a lot. I do like "Terminator 1" as well, but "Terminator 2" has a slight edge in terms of its surface properties.

When asked about the possibility of Skynet, the autonomous weapon system from "Terminator", becoming a reality, I believe it is a possibility. I am a hundred percent worried about AI being used for war. These fears of AGIs and how they will pan out are valid. These will be very powerful entities at some point. For a long time, they are going to be tools in the hands of humans. People talk about alignment of AGIs and how to make them align with human values. The problem is even humans are not aligned. So how this will be used and what this is going to look like is troubling.

My hope is that the development of AGIs happens slowly enough and in an open enough way where a lot of people can see and participate in it. We need to figure out how to deal with this transition, which is going to be interesting.

I draw a lot of inspiration from nuclear weapons. I thought that once they developed nuclear weapons, we would be doomed. But it's almost like when the systems are not so dangerous that they destroy human civilization, we deploy them, learn the lessons, and then quickly, if it's too dangerous, we quickly learn not to use them. There will be a balance achieved. Humans are very clever as a species. It's interesting, we exploit situations to our advantage.
# Discussion on Humanity's Future and Technology

We are constantly utilizing resources as much as we can, but we seem to avoid destroying ourselves. 

Well, I'm not entirely sure about that. I hope it continues. I'm definitely concerned about nuclear weapons and so on, not just as a result of the recent conflict, but even before that. That's probably my number one concern for humanity.

So, if humanity destroys itself or destroys 90% of people, would that be because of nuclear weapons? Yes, I think so. And it's not just about full destruction. To me, it's bad enough if we reset society. That would be terrible. That would be really bad. And I can't believe we're so close to it. 

It's like so crazy to me. It feels like we might be a few Tweets away from something like that. Basically, it's extremely unnerving and has been for me for a long time. 

It seems unstable that world leaders just having a bad mood can take one step towards a bad direction and then it escalates. And because of a collection of bad moods, it can escalate without being able to stop. It's a huge amount of power. 

And then also with the proliferation, I don't actually really see what the good outcomes are here, so I'm definitely worried about that a lot. 

And then there's AGI (Artificial General Intelligence). It's not currently there but I think at some point it will more and more become something like it. The danger with AGI even is that I think it's even slightly worse in the sense that there are good outcomes of AGI and then the bad outcomes are an epsilon way, like a tiny run away. 

And so I think capitalism and humanity, and so on, will drive for the positive ways of using that technology. But then if bad outcomes are just like a tiny, like flip a minus sign away, that's a really bad position to be in. A tiny perturbation of the system results in the destruction of the human species. It's a weird line to walk.

Yeah, I think in general what's really weird about the dynamics of humanity and this explosion we talked about is just the insane coupling afforded by technology. And just the instability of the whole dynamical system. I think it doesn't look good, honestly. 

That explosion could be destructive or constructive and the probabilities are non-zero in both ends of it. I'm going to have to try to be optimistic and so on and I think even in this case I still am predominantly optimistic but there's definitely a concern.

Do you think we'll become a multi-planetary species? Probably, yes. But I don't know if it's a dominant feature of future humanity.
# Discussion on Life on Mars and Virtual Reality

There might be some people on some planets, but I'm not sure if it's a major player in our culture. We still have to solve the drivers of self-destruction here on Earth. Just having a backup on Mars is not going to solve the problem.

By the way, I love the idea of a backup on Mars. I think that's amazing. We should absolutely do that. 

Would I go to Mars? Personally, no. I like Earth quite a lot. Maybe eventually I would, once it's safe enough. But I don't actually know if it's on my lifetime scale unless I can extend it by a lot. 

I do think that a lot of people might disappear into virtual realities. That could be the major thrust of the cultural development of humanity if it survives. It's just really hard to work in the physical realm and go out there. Ultimately, all your experiences are in your brain. 

It's much easier to disappear into the digital realm. I think people will find it more compelling, easier, safer, and more interesting. I'm a little bit captivated by virtual reality, by the possible worlds, whether it's the metaverse or some other manifestation of that.

To be clear, I think what's interesting about the future is that the variance in the human condition grows. That's the primary thing that's changing. It's not as much the mean of the distribution, it's the variance of it. There will probably be people on Mars, and there will be people in VR, and there will be people here on Earth. 

There will be so many more ways of being. I see it as a spreading out of the human experience. There's something about the internet that allows you to discover those little groups that you gravitate to. Something about your biology likes that kind of world, and you find each other. 

We'll have trans-humanists and then we'll have the Amish, and everything is just going to coexist. The cool thing about it is that these internet communities don't know about each other. You can have a very happy existence just having a very close-knit community and not knowing about the rest of the world. 

Even when you travel to places like Ukraine, they don't know so many things about America. When you travel across the world, I think you experience this too.
# The Future of Virtual Reality and the Power of Ideas

There are certain cultures that have their own unique characteristics. You can see this becoming more prevalent, and I believe this trend will continue to grow in the future. We have these little communities, and it seems to be the direction we're heading in. I don't see this trend reversing anytime soon. 

People are diverse and have the ability to choose their own path in existence, and I celebrate that. 

A question that often arises is whether I spend much time in the metaverse, in virtual reality. Or which community do I belong to? Am I a physicalist, someone who enjoys physical reality, or do I derive a lot of pleasure and fulfillment in the digital world?

Currently, I find that virtual reality is not that compelling. However, I do believe it can improve significantly, but to what extent, I'm not sure. Perhaps there are even more exotic possibilities we can explore with technologies like neural links. 

At present, I see myself as a human person. I love nature, harmony, people, and humanity. I love the emotions of humanity and I just want to be in this solar punk little utopia. That's my happy place. My happy place is being with people I love, thinking about cool problems, and being surrounded by a lush, beautiful, dynamic nature. 

I believe in using technology sparingly and in places that count, to empower my love for other humans and nature. I don't love when technology gets in the way of humanity. I prefer people being humans, slightly evolved in a way we naturally prefer.

People often ask me about my reading habits because they know I love reading. There are many books that have had an impact on me, for both silly and profound reasons. In biology, for example, "The Vital Question" is a good one. Anything by Nick Lane is worth reading, particularly "Life Ascending". 

"The Selfish Gene" was another book that had a significant impact on me. It helped me understand altruism and where it comes from. The realization that selection occurs at the level of genes was a huge insight for me and clarified many things.

When asked about the idea that ideas are the organisms, the memes, I wholeheartedly agree. I love the concept and believe in it a hundred percent.
# The Evolutionary Process of Ideas

There is an evolutionary kind of process with ideas as well. There are memes just like genes, and they compete and live in our brains. It's a beautiful concept.

Are we, as humans, silly for thinking that we are the organisms? Is it possible that the primary organisms are the ideas? 

Yes, I would say that the ideas kind of live in the software of our civilization, in our minds, and so on. We, as humans, tend to think that the hardware is the fundamental thing. A human is a hardware entity. But it could be the software, right?

Yes, I would say there needs to be some grounding at some point to a physical reality. But if we clone an Andrej, the software is the thing that makes that thing special, right? 

I guess you're right. But then cloning might be exceptionally difficult. There might be a deep integration between the software and the hardware in ways we don't quite yet understand.

Well, from the altruism point of view, what makes me special is more the gang of genes that are riding in my chromosomes, I suppose. They're the replicating unit, I suppose. 

No, but that's just the compute. The thing that makes you special, sure. Well, the reality is what makes you special is your ability to survive based on the software that runs on the hardware that was built by the genes. So the software is the thing that makes you survive. Not the hardware.

It's a little bit of both. It's just like a second layer. It's a new second layer that hasn't been there before the brain. They both coexist. But there's also layers of the software. I mean it's an abstraction on top of abstractions.

## Recommended Reading: "Selfish Gene" by Nick Lane

Sometimes, books are not sufficient. I like to reach for textbooks sometimes. I feel like books are for too much of a general consumption sometimes and they're too high up in the level of abstraction and it's not good enough.

That's why I like textbooks, I like "The Cell". I think "The Cell" was pretty cool. That's why also I like the writing of Nick Lane is because he's pretty willing to step one level down and he doesn't shy away from it. He's willing to go there but he's also willing to be throughout the stack. So he'll go down to a lot of detail but then he will come back up and I think he has a good balance. 

That's why I love college, early college, even high school, just textbooks on the basics of computer science, of mathematics, of biology, of chemistry. Those are, they condense down, it's sufficient in general that you can understand both the philosophy and the details but also the broader picture.
# The Future of AI and Synthetic Biology

In the world of learning, you get homework problems and you get to play with it as much as you would if you were in programming stuff. However, I'm also suspicious of textbooks. For instance, in deep learning, there are no amazing textbooks and the field is changing very quickly. I imagine the same is true in synthetic biology. Books like "The Cell" are kind of outdated. They're still high-level, but what is the actual real source of truth? It's people in wet labs working with cells, sequencing genomes, and actually working with it. 

I don't have much exposure to that or what that looks like. So, I'm reading through "The Cell" and it's kind of interesting, and I'm learning, but it's still not sufficient in terms of understanding. It's a clean summarization of the mainstream narrative. But you have to learn that before you break out towards the cutting edge. 

But what is the actual process of working with these cells, growing them, incubating them? It's like a massive cooking recipe. So, making sure your cells grow and proliferate, then you're sequencing them, running experiments, and just understanding how that works. I think that is the source of truth at the end of the day. It's what's really useful in terms of creating therapies and so on.

I wonder what in the future AI textbooks will be. There's "Artificial Intelligence: A Modern Approach". I haven't read the recent version, there's been a recent edition. I also saw there's a "Science of Deep Learning" book. I'm waiting for textbooks that are worth recommending, worth reading. 

It's tricky because it's like papers and code, code, code. Honestly, I find papers are quite good. I especially like the appendix of any paper as well. It's like the most detail you can have. It doesn't have to be cohesive, connected to anything else. You just described a very specific way you saw a particular thing. 

Many times, papers can be quite readable. The introduction and the abstract are readable even for someone outside of the field. This is not always true and sometimes I think, unfortunately, scientists use complex terms even when it's not necessary. I think that's harmful. There's no reason for that. 

Papers sometimes are longer than they need to be in the parts that don't matter. The appendix should be long, but then the papers themselves should be simple. Look at Einstein, make it simple. But certainly, I've come to appreciate the value of papers in learning.
# Across Papers and Synthetic Biology

I would say, across papers, say like synthetic biology or something that I thought were quite readable for the abstract and the introduction. Then you're reading the rest of it and you don't fully understand, but you kind of are getting a gist and I think it's cool.

## Advice to Folks Interested in Machine Learning

You give advice to folks interested in machine learning and research but in general life advice to a young person, high school, early college about how to have a career they can be proud of or a life they can be proud of?

Yeah, I think I'm very hesitant to give general advice. I think it's really hard. I've mentioned, some of the stuff I've mentioned is fairly general, I think. Like focus on just the amount of work you're spending on like a thing. Compare yourself only to yourself, not to others. 

## How to Pick the Thing?

How do you pick the thing? You just have like a deep interest in something or try to find the argmax over the things that you're interested in. Argmax at that moment and stick with it.

## How to Not Get Distracted?

How do you not get distracted and switch to another thing? You can if you like. Well, if you do an argmax repeatedly every week, every month, it doesn't converge. It's a problem. 

You can like low-pass filter yourself in terms of what has consistently been true for you. But yeah, I definitely see how it can be hard but I would say you're going to work the hardest on the thing that you care about the most. 

So low pass filter yourself and really introspect in your past, what are the things that gave you energy and what are the things that took energy away from you? Concrete examples. And usually, from those concrete examples, sometimes patterns can merge. I like it when things look like this when I'm in these positions. 

So that's not necessarily the field but the kind of stuff you're doing in a particular field. So for you, it seems like you were energized by implementing stuff, building actual things.

## Being Low-Level, Learning and Communicating

Yeah, being low-level, learning and then also communicating so that others can go through the same realizations and shortening that gap. Because I usually have to do way too much work to understand a thing and then I'm like, okay, this is actually like, okay, I think I get it and like why was it so much work? It should have been much less work. And that gives me a lot of frustration and that's why I sometimes go teach.

## The Future for Andrej Karpathy

So aside from the teaching you're doing now, putting out videos, aside from a potential "Godfather II" would the AGI at Tesla and beyond, what does the future for Andrej Karpathy hold, have you figured that out yet or no? I mean as you see through the fog of war that is all of our future, do you start seeing silhouettes of what that possible.
# The Future of AI: A Conversation with Andrej Karpathy

The consistent thing I've always been interested in is Artificial Intelligence (AI). For me, at least, that's probably what I'm spending the rest of my life on because I care about it a lot. I also care about many other problems, like aging, which I view as a disease. However, I don't think it's a good idea to go after it specifically. 

I don't actually think that humans will be able to come up with the answer. I believe the correct thing to do is to ignore those problems, solve AI, and then use that to solve everything else. I think there's a high chance that this approach will work, and that's the way I'm betting, at least.

So, when you think about AI, are you interested in all kinds of applications and domains? Yes, I am. Any domain you focus on will allow you to gain insights into the big problem of Artificial General Intelligence (AGI). For me, it's the ultimate meta-problem. I don't want to work on any one specific problem, as there are too many problems. So, how can you work on all problems simultaneously? You solve the meta-problem, which to me is just intelligence and how to automate it.

There are always some fun side projects, like arxiv-sanity. There are way too many archive papers, so how can I organize them and recommend papers and so on? I even transcribed all of your podcasts. 

What did I learn from that experience, from transcribing the process of consuming audiobooks and podcasts? Well, I was definitely surprised that transcription with OpenAI Whisper was working so well compared to what I'm familiar with, from Siri and a few other systems. It works so well, and that's what gave me some energy to try it out. I thought it could be fun to run on podcasts.

It's not obvious to me why Whisper is so much better compared to anything else because I feel like there should be a lot of incentive for a lot of companies to produce transcription systems, and they've done so over a long time. Whisper is not a super exotic model, it's a transformer, it takes mel spectrograms and just outputs tokens of text. It's not crazy. The model and everything has been around for a long time. I'm not actually a hundred percent sure why this came about. 

It's not obvious to me either. It makes me feel like I'm missing something. Even Google and YouTube transcription systems are not as good. But some of it is also about integrating into a bigger system.
# AI and the Future of Content Creation

**Andrej**: Yeah, so the user interface, how it's deployed, and all that kind of stuff. Maybe running it as an independent thing is much easier, like an order of magnitude easier than deploying it to a large integrated system like YouTube transcription, or anything like meetings. Zoom has transcription, but it's kind of crappy. 

Creating an interface where it detects the different individual speakers and is able to display it in compelling ways, run it real-time, all that kind of stuff. Maybe that's difficult. That's the only explanation I have because I'm currently paying quite a bit for human transcription, human caption.

**Andrej**: Right.

**Speaker**: Annotation. And it seems like there's a huge incentive to automate that.

**Andrej**: Yeah. It's very confusing.

**Speaker**: And I think, I mean, I don't know if you looked at some of the Whisper transcripts, but they're quite good. They're good. And especially in tricky cases.

**Andrej**: Yeah.

**Speaker**: I've seen Whisper's performance on super tricky cases and it does incredibly well. So I don't know, a podcast is pretty simple. It's like high-quality audio and you're speaking usually pretty clearly.

**Andrej**: Yeah.

**Speaker**: And so I don't know, I don't know what OpenAI's plans are either. But yeah, there's always fun projects basically. And stable diffusion also is opening up a huge amount of experimentation I would say in the visual realm and generating images, and videos, and movies ultimately.

**Lex**: Yeah, videos now.

**Speaker**: And so that's going to be pretty crazy. That's going to almost certainly work and it's going to be really interesting when the cost of content creation is going to fall to zero. You used to need a painter for a few months to paint a thing and now it's going to be speak to your phone to get your video.

**Speaker**: So if Hollywood will start using that to generate scenes which completely opens up. Yeah. So you can make a movie like "Avatar" eventually for under a million dollars.

**Speaker**: Much less maybe just by talking to your phone. I mean, I know it sounds kind of crazy. And then there'd be some voting mechanism, would there be a show on Netflix that's generated completely automatically? Semi-automatically?

**Speaker**: Yeah, potentially. Yeah. And what does it look like also when you can just generate it on demand and there's infinity of it?

**Speaker**: Yeah. Oh, man. All the synthetic content. I mean it's humbling because we treat ourselves as special for being able to generate art, and ideas, and all that kind of stuff. If that can be done in an automated way by AI.

**Speaker**: Yeah. I think it's fascinating to me how these, the predictions of AI and what it's going to look like and what it's going to be capable of.
# AI and the Meaning of Life: A Conversation with Andrej Karpathy

The sci-fi of the fifties and sixties completely inverted and got it wrong. They imagined AI as super calculating theorem provers, but what we're getting are systems that can talk to you about emotions. They can even create art. It's just weird.

**Interviewer**: Are you excited about that future? Just AI's, like hybrid systems, heterogeneous systems of humans and AIs talking about emotions, Netflix and chill with an AI system. Or the Netflix thing you watch is also generated by AI?

**Andrej**: I think it's going to be interesting for sure and I'm cautiously optimistic, but it's not obvious.

**Interviewer**: Well, the sad thing is your brain and mine developed in a time before Twitter, before the internet. So I wonder if people that are born inside of it might have a different experience. Like I, and maybe you, will still resist it and the people born now will not.

**Andrej**: Well, I do feel like humans are extremely malleable. And you're probably right.

**Interviewer**: What is the meaning of life, Andrej? We talked about the universe having a conversation with us humans or with the systems we create to try to answer. For the creator of the universe to notice us, we're trying to create systems that are loud enough to answer back.

**Andrej**: I don't know if that's the meaning of life. That's like the meaning of life for some people. The first level answer I would say is anyone can choose their own meaning of life because we are a conscious entity and it's beautiful, number one. But I do think that a deeper meaning of life, if someone is interested, is along the lines of like, what the hell is all this? And like why?

If you look into fundamental physics and the quantum field theory and the standard model, they're very complicated. And there's these 19 free parameters of our universe and what's going on with all this stuff and why is it here? And can I hack it? Can I work with it? Is there a message for me? Am I supposed to create a message?

And so I think there's some fundamental answers there but I think there's actually even like, you can't really make a dent in those without more time. And so to me also there's a big question around just getting more time, honestly. Yeah. That's what I think about quite a bit as well.

**Interviewer**: So kind of the ultimate, or at least first way to sneak up to the 'why' question is to try to escape the system, the universe?

**Andrej**: Yeah.

**Interviewer**: And then for that you backtrack and say, okay, for that, that's gonna take a very long time. So the 'why' question boils down from an engineering perspective to how do we extend?

**Andrej**: Yeah. I think that's the question number one, practically speaking, because you're not gonna calculate the answer to the deeper questions in the time you have. And that could be extending your own lifetime or extending just the lifetime of human civilization.
# A Conversation with Andrej Karpathy

"Of whoever wants to, many people might not want that," I began. 

"Yeah," Lex agreed. 

"But I think people who do want that, I think it's probably possible, and I don't know that people fully realize this. I feel like people think of death as an inevitability but at the end of the day, this is a physical system. Some things go wrong. It makes sense why things like this happen, evolutionarily speaking, and there's most certainly interventions that mitigate it."

"That would be interesting if death is eventually looked at as a fascinating thing that used to happen to humans," I mused. 

"I don't think it's unlikely. I think it's likely. And it's up to our imagination to try to predict what the world without death looks like," Lex added.

"Yeah," Andrej chimed in. "It's hard to, I think the values will completely change."

"Could be, I don't really buy all these ideas that, oh, without death, there's no meaning, there's nothingness. I don't intuitively buy all those arguments. I think there's plenty of meaning, plenty of things to learn. They're interesting, exciting. I want to know, I want to calculate, I want to improve the condition of all the humans and organisms that are alive."

"Yeah," I agreed. "The way we find meaning might change. There is a lot of humans, probably including myself, that finds meaning in the finiteness of things, but that doesn't mean that's the only source of meaning."

"Yeah," Andrej agreed. "I do think many people will go with that, which I think is great. I love the idea that people can just choose their own adventure. You are born as a conscious, free entity by default. I'd like to think."

"Yeah," Lex agreed. "And you have your unalienable rights for life."

"In the pursuit of happiness? I don't know if you that, and the nature, the landscape of happiness. And you can choose your own adventure, mostly. And that's not fully true but."

"I'm still am pretty sure I'm an NPC, but an NPC can't know it's an NPC. There could be different degrees and levels of consciousness. I don't think there's a more beautiful way to end it. Andrej, you're an incredible person. I'm really honored you would talk with me, everything you've done for the machine learning world, for the AI world to just inspire people, to educate millions of people. It's been great and I can't wait to see what you do next. It's been an honor, man. Thank you so much for talking today."

"Awesome. Thank you," Andrej replied. 

"Thanks for listening to this conversation with Andrej Karpathy, to support this podcast please check out our sponsors in the description. And now, let me leave you some words from Samuel Karlin, 'The purpose of models is not to fit the data but to sharpen the questions.' Thanks for listening and hope to see you next time."